{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.8384 - loss: 0.3954 - val_accuracy: 0.9066 - val_loss: 0.2319\n",
      "Epoch 2/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 967us/step - accuracy: 0.9128 - loss: 0.2172 - val_accuracy: 0.9246 - val_loss: 0.1876\n",
      "Epoch 3/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 968us/step - accuracy: 0.9256 - loss: 0.1847 - val_accuracy: 0.9245 - val_loss: 0.1765\n",
      "Epoch 4/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9332 - loss: 0.1628 - val_accuracy: 0.9363 - val_loss: 0.1543\n",
      "Epoch 5/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9369 - loss: 0.1527 - val_accuracy: 0.9393 - val_loss: 0.1458\n",
      "Epoch 6/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9394 - loss: 0.1463 - val_accuracy: 0.9395 - val_loss: 0.1410\n",
      "Epoch 7/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9408 - loss: 0.1418 - val_accuracy: 0.9435 - val_loss: 0.1358\n",
      "Epoch 8/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 997us/step - accuracy: 0.9426 - loss: 0.1381 - val_accuracy: 0.9418 - val_loss: 0.1383\n",
      "Epoch 9/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 988us/step - accuracy: 0.9442 - loss: 0.1324 - val_accuracy: 0.9422 - val_loss: 0.1339\n",
      "Epoch 10/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 943us/step - accuracy: 0.9457 - loss: 0.1307 - val_accuracy: 0.9396 - val_loss: 0.1349\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 733us/step\n",
      "Acurácia: 0.936210348013551\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df_train = pd.read_csv('train_normalized.csv')\n",
    "df_test = pd.read_csv('test_normalized.csv')\n",
    "\n",
    "# Substitua esta parte pelo seu próprio código de preparação dos dados\n",
    "X_train = df_train.drop(columns=['satisfaction']).values\n",
    "y_train = df_train['satisfaction'].values\n",
    "X_test = df_test.drop(columns=['satisfaction']).values\n",
    "y_test = df_test['satisfaction'].values\n",
    "\n",
    "# Dividir o conjunto de treino em treino e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Definindo a arquitetura da MLP\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Camada de saída para classificação binária\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinar o modelo\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliar o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.8471 - loss: 0.3770 - val_accuracy: 0.9165 - val_loss: 0.2093\n",
      "Epoch 2/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9205 - loss: 0.1985 - val_accuracy: 0.9303 - val_loss: 0.1736\n",
      "Epoch 3/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9322 - loss: 0.1695 - val_accuracy: 0.9323 - val_loss: 0.1598\n",
      "Epoch 4/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 961us/step - accuracy: 0.9362 - loss: 0.1547 - val_accuracy: 0.9347 - val_loss: 0.1506\n",
      "Epoch 5/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9386 - loss: 0.1464 - val_accuracy: 0.9409 - val_loss: 0.1379\n",
      "Epoch 6/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9417 - loss: 0.1389 - val_accuracy: 0.9426 - val_loss: 0.1356\n",
      "Epoch 7/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9415 - loss: 0.1388 - val_accuracy: 0.9401 - val_loss: 0.1436\n",
      "Epoch 8/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9428 - loss: 0.1326 - val_accuracy: 0.9436 - val_loss: 0.1358\n",
      "Epoch 9/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 972us/step - accuracy: 0.9440 - loss: 0.1313 - val_accuracy: 0.9431 - val_loss: 0.1328\n",
      "Epoch 10/10\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9447 - loss: 0.1284 - val_accuracy: 0.9443 - val_loss: 0.1295\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 675us/step\n",
      "Acurácia: 0.9465275639051433\n"
     ]
    }
   ],
   "source": [
    "# Definindo a arquitetura da MLP\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Camada de saída para classificação binária\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinar o modelo\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliar o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar usar a learning rate de 0.001 e 20 epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.8518 - loss: 0.3660 - val_accuracy: 0.9162 - val_loss: 0.2031\n",
      "Epoch 2/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9218 - loss: 0.1949 - val_accuracy: 0.9323 - val_loss: 0.1665\n",
      "Epoch 3/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9329 - loss: 0.1638 - val_accuracy: 0.9353 - val_loss: 0.1540\n",
      "Epoch 4/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9362 - loss: 0.1526 - val_accuracy: 0.9408 - val_loss: 0.1439\n",
      "Epoch 5/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9407 - loss: 0.1431 - val_accuracy: 0.9403 - val_loss: 0.1450\n",
      "Epoch 6/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9426 - loss: 0.1390 - val_accuracy: 0.9408 - val_loss: 0.1396\n",
      "Epoch 7/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9419 - loss: 0.1350 - val_accuracy: 0.9396 - val_loss: 0.1386\n",
      "Epoch 8/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9438 - loss: 0.1316 - val_accuracy: 0.9366 - val_loss: 0.1423\n",
      "Epoch 9/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9459 - loss: 0.1269 - val_accuracy: 0.9411 - val_loss: 0.1292\n",
      "Epoch 10/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9469 - loss: 0.1257 - val_accuracy: 0.9467 - val_loss: 0.1242\n",
      "Epoch 11/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9463 - loss: 0.1254 - val_accuracy: 0.9446 - val_loss: 0.1331\n",
      "Epoch 12/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9472 - loss: 0.1240 - val_accuracy: 0.9412 - val_loss: 0.1255\n",
      "Epoch 13/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9487 - loss: 0.1191 - val_accuracy: 0.9484 - val_loss: 0.1210\n",
      "Epoch 14/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9488 - loss: 0.1185 - val_accuracy: 0.9483 - val_loss: 0.1209\n",
      "Epoch 15/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9485 - loss: 0.1178 - val_accuracy: 0.9492 - val_loss: 0.1236\n",
      "Epoch 16/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9509 - loss: 0.1150 - val_accuracy: 0.9488 - val_loss: 0.1180\n",
      "Epoch 17/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9505 - loss: 0.1146 - val_accuracy: 0.9498 - val_loss: 0.1158\n",
      "Epoch 18/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9526 - loss: 0.1113 - val_accuracy: 0.9458 - val_loss: 0.1315\n",
      "Epoch 19/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9505 - loss: 0.1116 - val_accuracy: 0.9484 - val_loss: 0.1164\n",
      "Epoch 20/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9514 - loss: 0.1101 - val_accuracy: 0.9519 - val_loss: 0.1135\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 781us/step\n",
      "Acurácia: 0.952071142593163\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "# Definindo um otimizador com learning rate específica\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar o mesmo exemplo com menos camadas escondidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.8312 - loss: 0.4002 - val_accuracy: 0.9096 - val_loss: 0.2253\n",
      "Epoch 2/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9133 - loss: 0.2171 - val_accuracy: 0.9234 - val_loss: 0.1909\n",
      "Epoch 3/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9272 - loss: 0.1841 - val_accuracy: 0.9309 - val_loss: 0.1734\n",
      "Epoch 4/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 985us/step - accuracy: 0.9310 - loss: 0.1682 - val_accuracy: 0.9339 - val_loss: 0.1599\n",
      "Epoch 5/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 992us/step - accuracy: 0.9360 - loss: 0.1574 - val_accuracy: 0.9369 - val_loss: 0.1497\n",
      "Epoch 6/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9369 - loss: 0.1501 - val_accuracy: 0.9322 - val_loss: 0.1649\n",
      "Epoch 7/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9385 - loss: 0.1456 - val_accuracy: 0.9420 - val_loss: 0.1397\n",
      "Epoch 8/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9392 - loss: 0.1418 - val_accuracy: 0.9417 - val_loss: 0.1391\n",
      "Epoch 9/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9428 - loss: 0.1354 - val_accuracy: 0.9398 - val_loss: 0.1367\n",
      "Epoch 10/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9434 - loss: 0.1348 - val_accuracy: 0.9454 - val_loss: 0.1310\n",
      "Epoch 11/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9456 - loss: 0.1305 - val_accuracy: 0.9464 - val_loss: 0.1285\n",
      "Epoch 12/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9449 - loss: 0.1290 - val_accuracy: 0.9467 - val_loss: 0.1293\n",
      "Epoch 13/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9467 - loss: 0.1257 - val_accuracy: 0.9444 - val_loss: 0.1276\n",
      "Epoch 14/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9466 - loss: 0.1251 - val_accuracy: 0.9450 - val_loss: 0.1256\n",
      "Epoch 15/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9472 - loss: 0.1229 - val_accuracy: 0.9430 - val_loss: 0.1286\n",
      "Epoch 16/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9480 - loss: 0.1223 - val_accuracy: 0.9484 - val_loss: 0.1210\n",
      "Epoch 17/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9511 - loss: 0.1187 - val_accuracy: 0.9443 - val_loss: 0.1229\n",
      "Epoch 18/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9491 - loss: 0.1185 - val_accuracy: 0.9429 - val_loss: 0.1333\n",
      "Epoch 19/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9474 - loss: 0.1199 - val_accuracy: 0.9452 - val_loss: 0.1282\n",
      "Epoch 20/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9498 - loss: 0.1186 - val_accuracy: 0.9513 - val_loss: 0.1180\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 710us/step\n",
      "Acurácia: 0.9524176162611642\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "# Definindo um otimizador com learning rate específica\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos chegar no mesmo resultado com menos camadas escondidas, vamos tentar reduzir mais uma vez para ver como fica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.8180 - loss: 0.4269 - val_accuracy: 0.8958 - val_loss: 0.2648\n",
      "Epoch 2/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9004 - loss: 0.2521 - val_accuracy: 0.9098 - val_loss: 0.2203\n",
      "Epoch 3/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9140 - loss: 0.2152 - val_accuracy: 0.9183 - val_loss: 0.2015\n",
      "Epoch 4/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9210 - loss: 0.1980 - val_accuracy: 0.9185 - val_loss: 0.1948\n",
      "Epoch 5/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9249 - loss: 0.1864 - val_accuracy: 0.9265 - val_loss: 0.1823\n",
      "Epoch 6/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9270 - loss: 0.1841 - val_accuracy: 0.9279 - val_loss: 0.1756\n",
      "Epoch 7/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9285 - loss: 0.1768 - val_accuracy: 0.9299 - val_loss: 0.1675\n",
      "Epoch 8/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9317 - loss: 0.1677 - val_accuracy: 0.9320 - val_loss: 0.1651\n",
      "Epoch 9/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9352 - loss: 0.1585 - val_accuracy: 0.9357 - val_loss: 0.1580\n",
      "Epoch 10/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9359 - loss: 0.1575 - val_accuracy: 0.9353 - val_loss: 0.1534\n",
      "Epoch 11/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9381 - loss: 0.1516 - val_accuracy: 0.9383 - val_loss: 0.1487\n",
      "Epoch 12/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9377 - loss: 0.1510 - val_accuracy: 0.9373 - val_loss: 0.1478\n",
      "Epoch 13/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9400 - loss: 0.1453 - val_accuracy: 0.9371 - val_loss: 0.1472\n",
      "Epoch 14/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9407 - loss: 0.1432 - val_accuracy: 0.9402 - val_loss: 0.1433\n",
      "Epoch 15/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9404 - loss: 0.1434 - val_accuracy: 0.9410 - val_loss: 0.1405\n",
      "Epoch 16/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9420 - loss: 0.1388 - val_accuracy: 0.9398 - val_loss: 0.1396\n",
      "Epoch 17/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9424 - loss: 0.1379 - val_accuracy: 0.9326 - val_loss: 0.1482\n",
      "Epoch 18/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9422 - loss: 0.1395 - val_accuracy: 0.9424 - val_loss: 0.1384\n",
      "Epoch 19/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9433 - loss: 0.1365 - val_accuracy: 0.9409 - val_loss: 0.1372\n",
      "Epoch 20/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9437 - loss: 0.1336 - val_accuracy: 0.9410 - val_loss: 0.1357\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707us/step\n",
      "Acurácia: 0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "# Definindo um otimizador com learning rate específica\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chegamos a conclusão que 64 hidden layers é o numero ideal, agora vamos tentar mudar o batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8240 - loss: 0.4306 - val_accuracy: 0.9045 - val_loss: 0.2431\n",
      "Epoch 2/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9096 - loss: 0.2275 - val_accuracy: 0.9219 - val_loss: 0.1985\n",
      "Epoch 3/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9254 - loss: 0.1903 - val_accuracy: 0.9275 - val_loss: 0.1794\n",
      "Epoch 4/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9306 - loss: 0.1744 - val_accuracy: 0.9326 - val_loss: 0.1641\n",
      "Epoch 5/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9321 - loss: 0.1647 - val_accuracy: 0.9365 - val_loss: 0.1544\n",
      "Epoch 6/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9378 - loss: 0.1539 - val_accuracy: 0.9364 - val_loss: 0.1501\n",
      "Epoch 7/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9389 - loss: 0.1487 - val_accuracy: 0.9401 - val_loss: 0.1447\n",
      "Epoch 8/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9407 - loss: 0.1438 - val_accuracy: 0.9357 - val_loss: 0.1585\n",
      "Epoch 9/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9426 - loss: 0.1397 - val_accuracy: 0.9425 - val_loss: 0.1400\n",
      "Epoch 10/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987us/step - accuracy: 0.9411 - loss: 0.1399 - val_accuracy: 0.9430 - val_loss: 0.1374\n",
      "Epoch 11/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9434 - loss: 0.1349 - val_accuracy: 0.9435 - val_loss: 0.1337\n",
      "Epoch 12/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 980us/step - accuracy: 0.9428 - loss: 0.1358 - val_accuracy: 0.9427 - val_loss: 0.1352\n",
      "Epoch 13/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 997us/step - accuracy: 0.9436 - loss: 0.1319 - val_accuracy: 0.9418 - val_loss: 0.1383\n",
      "Epoch 14/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9452 - loss: 0.1284 - val_accuracy: 0.9421 - val_loss: 0.1313\n",
      "Epoch 15/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - accuracy: 0.9446 - loss: 0.1297 - val_accuracy: 0.9429 - val_loss: 0.1333\n",
      "Epoch 16/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9470 - loss: 0.1262 - val_accuracy: 0.9437 - val_loss: 0.1295\n",
      "Epoch 17/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9460 - loss: 0.1263 - val_accuracy: 0.9451 - val_loss: 0.1263\n",
      "Epoch 18/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9471 - loss: 0.1236 - val_accuracy: 0.9455 - val_loss: 0.1262\n",
      "Epoch 19/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9481 - loss: 0.1233 - val_accuracy: 0.9422 - val_loss: 0.1261\n",
      "Epoch 20/20\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9481 - loss: 0.1244 - val_accuracy: 0.9479 - val_loss: 0.1225\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686us/step\n",
      "Acurácia: 0.9489143825069295\n"
     ]
    }
   ],
   "source": [
    "# Definindo um otimizador com learning rate específica\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O batchsize maior não surtiu efeito, vamos tentar aumentar o numero de epocas mantendo esse batch size, qualquer coisa revertemos posteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8084 - loss: 0.4433 - val_accuracy: 0.9055 - val_loss: 0.2433\n",
      "Epoch 2/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9086 - loss: 0.2287 - val_accuracy: 0.9162 - val_loss: 0.2056\n",
      "Epoch 3/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9241 - loss: 0.1922 - val_accuracy: 0.9259 - val_loss: 0.1802\n",
      "Epoch 4/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9304 - loss: 0.1749 - val_accuracy: 0.9326 - val_loss: 0.1660\n",
      "Epoch 5/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9333 - loss: 0.1637 - val_accuracy: 0.9359 - val_loss: 0.1574\n",
      "Epoch 6/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9362 - loss: 0.1575 - val_accuracy: 0.9287 - val_loss: 0.1613\n",
      "Epoch 7/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9377 - loss: 0.1503 - val_accuracy: 0.9379 - val_loss: 0.1506\n",
      "Epoch 8/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 981us/step - accuracy: 0.9399 - loss: 0.1464 - val_accuracy: 0.9393 - val_loss: 0.1438\n",
      "Epoch 9/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9406 - loss: 0.1426 - val_accuracy: 0.9402 - val_loss: 0.1433\n",
      "Epoch 10/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9420 - loss: 0.1382 - val_accuracy: 0.9418 - val_loss: 0.1372\n",
      "Epoch 11/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9430 - loss: 0.1367 - val_accuracy: 0.9427 - val_loss: 0.1332\n",
      "Epoch 12/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9458 - loss: 0.1321 - val_accuracy: 0.9427 - val_loss: 0.1330\n",
      "Epoch 13/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9452 - loss: 0.1314 - val_accuracy: 0.9435 - val_loss: 0.1334\n",
      "Epoch 14/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9456 - loss: 0.1289 - val_accuracy: 0.9442 - val_loss: 0.1303\n",
      "Epoch 15/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9465 - loss: 0.1276 - val_accuracy: 0.9423 - val_loss: 0.1313\n",
      "Epoch 16/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9455 - loss: 0.1275 - val_accuracy: 0.9440 - val_loss: 0.1332\n",
      "Epoch 17/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9477 - loss: 0.1249 - val_accuracy: 0.9454 - val_loss: 0.1291\n",
      "Epoch 18/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9481 - loss: 0.1228 - val_accuracy: 0.9461 - val_loss: 0.1255\n",
      "Epoch 19/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9483 - loss: 0.1233 - val_accuracy: 0.9480 - val_loss: 0.1232\n",
      "Epoch 20/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9492 - loss: 0.1195 - val_accuracy: 0.9475 - val_loss: 0.1224\n",
      "Epoch 21/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9473 - loss: 0.1228 - val_accuracy: 0.9475 - val_loss: 0.1239\n",
      "Epoch 22/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9483 - loss: 0.1208 - val_accuracy: 0.9464 - val_loss: 0.1219\n",
      "Epoch 23/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9488 - loss: 0.1200 - val_accuracy: 0.9479 - val_loss: 0.1224\n",
      "Epoch 24/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9503 - loss: 0.1180 - val_accuracy: 0.9483 - val_loss: 0.1223\n",
      "Epoch 25/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9499 - loss: 0.1172 - val_accuracy: 0.9495 - val_loss: 0.1204\n",
      "Epoch 26/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9510 - loss: 0.1166 - val_accuracy: 0.9482 - val_loss: 0.1204\n",
      "Epoch 27/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9498 - loss: 0.1165 - val_accuracy: 0.9469 - val_loss: 0.1188\n",
      "Epoch 28/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9499 - loss: 0.1168 - val_accuracy: 0.9486 - val_loss: 0.1185\n",
      "Epoch 29/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9497 - loss: 0.1155 - val_accuracy: 0.9487 - val_loss: 0.1196\n",
      "Epoch 30/30\n",
      "\u001b[1m1380/1380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9520 - loss: 0.1130 - val_accuracy: 0.9488 - val_loss: 0.1167\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971us/step\n",
      "Acurácia: 0.9499923005851555\n"
     ]
    }
   ],
   "source": [
    "# Definindo um otimizador com learning rate específica\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=64, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado não passou de 0.95, ou seja, o batchsize maior piorou o modelo, vamos testar esse mesmo número de epocas mas com batchsize de 32, que é o numero ideal de batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.8328 - loss: 0.4040 - val_accuracy: 0.9066 - val_loss: 0.2401\n",
      "Epoch 2/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9105 - loss: 0.2247 - val_accuracy: 0.9214 - val_loss: 0.1940\n",
      "Epoch 3/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9230 - loss: 0.1917 - val_accuracy: 0.9245 - val_loss: 0.1873\n",
      "Epoch 4/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9306 - loss: 0.1759 - val_accuracy: 0.9294 - val_loss: 0.1747\n",
      "Epoch 5/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9332 - loss: 0.1679 - val_accuracy: 0.9333 - val_loss: 0.1646\n",
      "Epoch 6/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1000us/step - accuracy: 0.9362 - loss: 0.1599 - val_accuracy: 0.9356 - val_loss: 0.1573\n",
      "Epoch 7/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9383 - loss: 0.1551 - val_accuracy: 0.9362 - val_loss: 0.1543\n",
      "Epoch 8/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9389 - loss: 0.1535 - val_accuracy: 0.9371 - val_loss: 0.1539\n",
      "Epoch 9/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9402 - loss: 0.1478 - val_accuracy: 0.9389 - val_loss: 0.1472\n",
      "Epoch 10/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9403 - loss: 0.1479 - val_accuracy: 0.9369 - val_loss: 0.1495\n",
      "Epoch 11/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 981us/step - accuracy: 0.9433 - loss: 0.1414 - val_accuracy: 0.9302 - val_loss: 0.1679\n",
      "Epoch 12/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9415 - loss: 0.1430 - val_accuracy: 0.9392 - val_loss: 0.1443\n",
      "Epoch 13/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9431 - loss: 0.1396 - val_accuracy: 0.9360 - val_loss: 0.1531\n",
      "Epoch 14/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9412 - loss: 0.1400 - val_accuracy: 0.9425 - val_loss: 0.1404\n",
      "Epoch 15/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9433 - loss: 0.1378 - val_accuracy: 0.9401 - val_loss: 0.1419\n",
      "Epoch 16/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9433 - loss: 0.1358 - val_accuracy: 0.9446 - val_loss: 0.1364\n",
      "Epoch 17/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9454 - loss: 0.1336 - val_accuracy: 0.9451 - val_loss: 0.1338\n",
      "Epoch 18/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9436 - loss: 0.1355 - val_accuracy: 0.9433 - val_loss: 0.1350\n",
      "Epoch 19/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9452 - loss: 0.1318 - val_accuracy: 0.9427 - val_loss: 0.1363\n",
      "Epoch 20/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9452 - loss: 0.1281 - val_accuracy: 0.9372 - val_loss: 0.1422\n",
      "Epoch 21/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9475 - loss: 0.1238 - val_accuracy: 0.9466 - val_loss: 0.1275\n",
      "Epoch 22/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9477 - loss: 0.1239 - val_accuracy: 0.9477 - val_loss: 0.1248\n",
      "Epoch 23/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9468 - loss: 0.1257 - val_accuracy: 0.9466 - val_loss: 0.1283\n",
      "Epoch 24/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9475 - loss: 0.1251 - val_accuracy: 0.9474 - val_loss: 0.1228\n",
      "Epoch 25/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9498 - loss: 0.1203 - val_accuracy: 0.9433 - val_loss: 0.1315\n",
      "Epoch 26/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9497 - loss: 0.1173 - val_accuracy: 0.9487 - val_loss: 0.1237\n",
      "Epoch 27/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9492 - loss: 0.1196 - val_accuracy: 0.9477 - val_loss: 0.1234\n",
      "Epoch 28/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9496 - loss: 0.1215 - val_accuracy: 0.9478 - val_loss: 0.1223\n",
      "Epoch 29/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9490 - loss: 0.1180 - val_accuracy: 0.9490 - val_loss: 0.1207\n",
      "Epoch 30/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9499 - loss: 0.1165 - val_accuracy: 0.9486 - val_loss: 0.1195\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717us/step\n",
      "Acurácia: 0.9498768093624884\n"
     ]
    }
   ],
   "source": [
    "# Definindo um otimizador com learning rate específica\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda não conseguimos chegar ao melhore resultado\n",
    "Mantendo esses parametros vamos tentar usar 128 hidden layers com dropout de 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.8342 - loss: 0.3849 - val_accuracy: 0.9136 - val_loss: 0.2146\n",
      "Epoch 2/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9104 - loss: 0.2226 - val_accuracy: 0.9258 - val_loss: 0.1826\n",
      "Epoch 3/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9241 - loss: 0.1888 - val_accuracy: 0.9310 - val_loss: 0.1642\n",
      "Epoch 4/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9294 - loss: 0.1706 - val_accuracy: 0.9348 - val_loss: 0.1560\n",
      "Epoch 5/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9327 - loss: 0.1595 - val_accuracy: 0.9361 - val_loss: 0.1490\n",
      "Epoch 6/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9351 - loss: 0.1532 - val_accuracy: 0.9412 - val_loss: 0.1408\n",
      "Epoch 7/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9372 - loss: 0.1464 - val_accuracy: 0.9422 - val_loss: 0.1366\n",
      "Epoch 8/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9407 - loss: 0.1431 - val_accuracy: 0.9323 - val_loss: 0.1504\n",
      "Epoch 9/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9394 - loss: 0.1414 - val_accuracy: 0.9449 - val_loss: 0.1314\n",
      "Epoch 10/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9419 - loss: 0.1392 - val_accuracy: 0.9429 - val_loss: 0.1307\n",
      "Epoch 11/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9426 - loss: 0.1383 - val_accuracy: 0.9459 - val_loss: 0.1275\n",
      "Epoch 12/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9434 - loss: 0.1343 - val_accuracy: 0.9432 - val_loss: 0.1314\n",
      "Epoch 13/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9414 - loss: 0.1350 - val_accuracy: 0.9387 - val_loss: 0.1464\n",
      "Epoch 14/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.9428 - loss: 0.1337 - val_accuracy: 0.9376 - val_loss: 0.1375\n",
      "Epoch 15/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9446 - loss: 0.1303 - val_accuracy: 0.9474 - val_loss: 0.1244\n",
      "Epoch 16/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9445 - loss: 0.1297 - val_accuracy: 0.9467 - val_loss: 0.1231\n",
      "Epoch 17/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9439 - loss: 0.1307 - val_accuracy: 0.9469 - val_loss: 0.1222\n",
      "Epoch 18/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9462 - loss: 0.1277 - val_accuracy: 0.9474 - val_loss: 0.1252\n",
      "Epoch 19/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9464 - loss: 0.1262 - val_accuracy: 0.9448 - val_loss: 0.1235\n",
      "Epoch 20/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9467 - loss: 0.1234 - val_accuracy: 0.9482 - val_loss: 0.1218\n",
      "Epoch 21/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9465 - loss: 0.1241 - val_accuracy: 0.9449 - val_loss: 0.1219\n",
      "Epoch 22/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9463 - loss: 0.1241 - val_accuracy: 0.9498 - val_loss: 0.1179\n",
      "Epoch 23/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9471 - loss: 0.1250 - val_accuracy: 0.9407 - val_loss: 0.1267\n",
      "Epoch 24/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9469 - loss: 0.1244 - val_accuracy: 0.9472 - val_loss: 0.1250\n",
      "Epoch 25/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9483 - loss: 0.1234 - val_accuracy: 0.9478 - val_loss: 0.1189\n",
      "Epoch 26/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9480 - loss: 0.1228 - val_accuracy: 0.9490 - val_loss: 0.1216\n",
      "Epoch 27/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9479 - loss: 0.1210 - val_accuracy: 0.9495 - val_loss: 0.1201\n",
      "Epoch 28/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9491 - loss: 0.1211 - val_accuracy: 0.9510 - val_loss: 0.1169\n",
      "Epoch 29/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9479 - loss: 0.1225 - val_accuracy: 0.9506 - val_loss: 0.1166\n",
      "Epoch 30/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9468 - loss: 0.1241 - val_accuracy: 0.9503 - val_loss: 0.1167\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724us/step\n",
      "Acurácia: 0.951647674776717\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Definindo um otimizador com learning rate específica\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado com dropout e numero elevado de hidden layers não se pagou, pois mesmo chegando perto do resultado ótimo, o custo computacional foi muito maior, como no teste anterior a esse, a acuracia mudou muito pouco durante muitas epocas, vamos retornar à 64 camadas mas tentar incluir um dropout de 20% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.8117 - loss: 0.4270 - val_accuracy: 0.8963 - val_loss: 0.2574\n",
      "Epoch 2/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8991 - loss: 0.2539 - val_accuracy: 0.9169 - val_loss: 0.2044\n",
      "Epoch 3/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9151 - loss: 0.2141 - val_accuracy: 0.9283 - val_loss: 0.1815\n",
      "Epoch 4/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9225 - loss: 0.1916 - val_accuracy: 0.9325 - val_loss: 0.1643\n",
      "Epoch 5/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9292 - loss: 0.1765 - val_accuracy: 0.9360 - val_loss: 0.1551\n",
      "Epoch 6/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9298 - loss: 0.1700 - val_accuracy: 0.9365 - val_loss: 0.1511\n",
      "Epoch 7/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9335 - loss: 0.1626 - val_accuracy: 0.9390 - val_loss: 0.1474\n",
      "Epoch 8/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9341 - loss: 0.1587 - val_accuracy: 0.9398 - val_loss: 0.1445\n",
      "Epoch 9/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9371 - loss: 0.1526 - val_accuracy: 0.9414 - val_loss: 0.1401\n",
      "Epoch 10/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9382 - loss: 0.1499 - val_accuracy: 0.9425 - val_loss: 0.1385\n",
      "Epoch 11/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9378 - loss: 0.1492 - val_accuracy: 0.9438 - val_loss: 0.1347\n",
      "Epoch 12/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9378 - loss: 0.1488 - val_accuracy: 0.9459 - val_loss: 0.1339\n",
      "Epoch 13/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9399 - loss: 0.1433 - val_accuracy: 0.9442 - val_loss: 0.1321\n",
      "Epoch 14/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9400 - loss: 0.1445 - val_accuracy: 0.9445 - val_loss: 0.1302\n",
      "Epoch 15/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9397 - loss: 0.1420 - val_accuracy: 0.9456 - val_loss: 0.1300\n",
      "Epoch 16/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 999us/step - accuracy: 0.9419 - loss: 0.1400 - val_accuracy: 0.9460 - val_loss: 0.1280\n",
      "Epoch 17/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9435 - loss: 0.1356 - val_accuracy: 0.9468 - val_loss: 0.1259\n",
      "Epoch 18/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9416 - loss: 0.1398 - val_accuracy: 0.9447 - val_loss: 0.1279\n",
      "Epoch 19/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9426 - loss: 0.1357 - val_accuracy: 0.9465 - val_loss: 0.1267\n",
      "Epoch 20/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9422 - loss: 0.1367 - val_accuracy: 0.9477 - val_loss: 0.1246\n",
      "Epoch 21/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9415 - loss: 0.1370 - val_accuracy: 0.9478 - val_loss: 0.1253\n",
      "Epoch 22/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9441 - loss: 0.1337 - val_accuracy: 0.9483 - val_loss: 0.1237\n",
      "Epoch 23/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9437 - loss: 0.1348 - val_accuracy: 0.9425 - val_loss: 0.1264\n",
      "Epoch 24/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9441 - loss: 0.1330 - val_accuracy: 0.9493 - val_loss: 0.1225\n",
      "Epoch 25/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9448 - loss: 0.1318 - val_accuracy: 0.9480 - val_loss: 0.1227\n",
      "Epoch 26/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 993us/step - accuracy: 0.9436 - loss: 0.1313 - val_accuracy: 0.9490 - val_loss: 0.1229\n",
      "Epoch 27/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9431 - loss: 0.1339 - val_accuracy: 0.9481 - val_loss: 0.1219\n",
      "Epoch 28/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9450 - loss: 0.1320 - val_accuracy: 0.9489 - val_loss: 0.1222\n",
      "Epoch 29/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9444 - loss: 0.1312 - val_accuracy: 0.9500 - val_loss: 0.1198\n",
      "Epoch 30/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9453 - loss: 0.1291 - val_accuracy: 0.9478 - val_loss: 0.1241\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 579us/step\n",
      "Acurácia: 0.9501077918078226\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Definindo um otimizador com learning rate específica\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não valeu a pena, ainda não chegamos no melhor resultado, vamos tentar diminuir as épocas com esses parametros para ver se o dropout faz algum efeito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.8155 - loss: 0.4223 - val_accuracy: 0.9068 - val_loss: 0.2336\n",
      "Epoch 2/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9049 - loss: 0.2334 - val_accuracy: 0.9227 - val_loss: 0.1926\n",
      "Epoch 3/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9211 - loss: 0.1995 - val_accuracy: 0.9290 - val_loss: 0.1762\n",
      "Epoch 4/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9274 - loss: 0.1805 - val_accuracy: 0.9349 - val_loss: 0.1618\n",
      "Epoch 5/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9319 - loss: 0.1670 - val_accuracy: 0.9375 - val_loss: 0.1497\n",
      "Epoch 6/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9330 - loss: 0.1627 - val_accuracy: 0.9395 - val_loss: 0.1464\n",
      "Epoch 7/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9365 - loss: 0.1548 - val_accuracy: 0.9349 - val_loss: 0.1490\n",
      "Epoch 8/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9381 - loss: 0.1511 - val_accuracy: 0.9401 - val_loss: 0.1388\n",
      "Epoch 9/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 997us/step - accuracy: 0.9391 - loss: 0.1471 - val_accuracy: 0.9411 - val_loss: 0.1401\n",
      "Epoch 10/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9385 - loss: 0.1489 - val_accuracy: 0.9408 - val_loss: 0.1353\n",
      "Epoch 11/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9406 - loss: 0.1446 - val_accuracy: 0.9436 - val_loss: 0.1317\n",
      "Epoch 12/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9422 - loss: 0.1405 - val_accuracy: 0.9397 - val_loss: 0.1358\n",
      "Epoch 13/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9422 - loss: 0.1404 - val_accuracy: 0.9430 - val_loss: 0.1382\n",
      "Epoch 14/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9416 - loss: 0.1385 - val_accuracy: 0.9453 - val_loss: 0.1289\n",
      "Epoch 15/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9423 - loss: 0.1385 - val_accuracy: 0.9469 - val_loss: 0.1278\n",
      "Epoch 16/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9436 - loss: 0.1354 - val_accuracy: 0.9439 - val_loss: 0.1289\n",
      "Epoch 17/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9432 - loss: 0.1357 - val_accuracy: 0.9429 - val_loss: 0.1313\n",
      "Epoch 18/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9439 - loss: 0.1336 - val_accuracy: 0.9439 - val_loss: 0.1292\n",
      "Epoch 19/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9430 - loss: 0.1347 - val_accuracy: 0.9458 - val_loss: 0.1255\n",
      "Epoch 20/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9429 - loss: 0.1317 - val_accuracy: 0.9468 - val_loss: 0.1241\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 739us/step\n",
      "Acurácia: 0.9488758854327071\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Definindo um otimizador com learning rate específica\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dropout não se pagou. Vamos tentar com nosso melhor modelo, diminuir a learning rate e dobrar o número de épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.8071 - loss: 0.4496 - val_accuracy: 0.8954 - val_loss: 0.2657\n",
      "Epoch 2/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9018 - loss: 0.2476 - val_accuracy: 0.9135 - val_loss: 0.2150\n",
      "Epoch 3/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9172 - loss: 0.2085 - val_accuracy: 0.9228 - val_loss: 0.1947\n",
      "Epoch 4/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9251 - loss: 0.1899 - val_accuracy: 0.9299 - val_loss: 0.1760\n",
      "Epoch 5/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9322 - loss: 0.1719 - val_accuracy: 0.9332 - val_loss: 0.1654\n",
      "Epoch 6/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9351 - loss: 0.1635 - val_accuracy: 0.9355 - val_loss: 0.1580\n",
      "Epoch 7/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9353 - loss: 0.1587 - val_accuracy: 0.9372 - val_loss: 0.1528\n",
      "Epoch 8/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9370 - loss: 0.1533 - val_accuracy: 0.9391 - val_loss: 0.1499\n",
      "Epoch 9/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9385 - loss: 0.1470 - val_accuracy: 0.9400 - val_loss: 0.1449\n",
      "Epoch 10/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9402 - loss: 0.1452 - val_accuracy: 0.9414 - val_loss: 0.1422\n",
      "Epoch 11/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9426 - loss: 0.1406 - val_accuracy: 0.9412 - val_loss: 0.1408\n",
      "Epoch 12/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9430 - loss: 0.1386 - val_accuracy: 0.9402 - val_loss: 0.1386\n",
      "Epoch 13/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9428 - loss: 0.1374 - val_accuracy: 0.9432 - val_loss: 0.1369\n",
      "Epoch 14/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9432 - loss: 0.1363 - val_accuracy: 0.9438 - val_loss: 0.1337\n",
      "Epoch 15/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9460 - loss: 0.1302 - val_accuracy: 0.9439 - val_loss: 0.1320\n",
      "Epoch 16/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9458 - loss: 0.1287 - val_accuracy: 0.9398 - val_loss: 0.1378\n",
      "Epoch 17/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9457 - loss: 0.1305 - val_accuracy: 0.9357 - val_loss: 0.1413\n",
      "Epoch 18/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9471 - loss: 0.1259 - val_accuracy: 0.9411 - val_loss: 0.1352\n",
      "Epoch 19/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9452 - loss: 0.1287 - val_accuracy: 0.9456 - val_loss: 0.1285\n",
      "Epoch 20/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9477 - loss: 0.1249 - val_accuracy: 0.9391 - val_loss: 0.1321\n",
      "Epoch 21/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9478 - loss: 0.1220 - val_accuracy: 0.9480 - val_loss: 0.1242\n",
      "Epoch 22/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9482 - loss: 0.1222 - val_accuracy: 0.9412 - val_loss: 0.1296\n",
      "Epoch 23/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9476 - loss: 0.1249 - val_accuracy: 0.9449 - val_loss: 0.1276\n",
      "Epoch 24/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9481 - loss: 0.1230 - val_accuracy: 0.9486 - val_loss: 0.1217\n",
      "Epoch 25/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9490 - loss: 0.1203 - val_accuracy: 0.9423 - val_loss: 0.1264\n",
      "Epoch 26/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9510 - loss: 0.1168 - val_accuracy: 0.9452 - val_loss: 0.1226\n",
      "Epoch 27/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9493 - loss: 0.1203 - val_accuracy: 0.9469 - val_loss: 0.1226\n",
      "Epoch 28/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9489 - loss: 0.1203 - val_accuracy: 0.9472 - val_loss: 0.1222\n",
      "Epoch 29/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9483 - loss: 0.1187 - val_accuracy: 0.9484 - val_loss: 0.1203\n",
      "Epoch 30/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9501 - loss: 0.1178 - val_accuracy: 0.9465 - val_loss: 0.1216\n",
      "Epoch 31/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9499 - loss: 0.1163 - val_accuracy: 0.9489 - val_loss: 0.1187\n",
      "Epoch 32/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9515 - loss: 0.1176 - val_accuracy: 0.9500 - val_loss: 0.1174\n",
      "Epoch 33/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9514 - loss: 0.1143 - val_accuracy: 0.9484 - val_loss: 0.1210\n",
      "Epoch 34/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9516 - loss: 0.1137 - val_accuracy: 0.9492 - val_loss: 0.1192\n",
      "Epoch 35/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9516 - loss: 0.1146 - val_accuracy: 0.9499 - val_loss: 0.1190\n",
      "Epoch 36/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9537 - loss: 0.1111 - val_accuracy: 0.9487 - val_loss: 0.1202\n",
      "Epoch 37/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9523 - loss: 0.1119 - val_accuracy: 0.9496 - val_loss: 0.1177\n",
      "Epoch 38/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 993us/step - accuracy: 0.9515 - loss: 0.1143 - val_accuracy: 0.9498 - val_loss: 0.1185\n",
      "Epoch 39/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 945us/step - accuracy: 0.9521 - loss: 0.1132 - val_accuracy: 0.9500 - val_loss: 0.1173\n",
      "Epoch 40/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 987us/step - accuracy: 0.9511 - loss: 0.1137 - val_accuracy: 0.9496 - val_loss: 0.1176\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 669us/step\n",
      "Acurácia: 0.951763165999384\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "# Definindo um otimizador com learning rate específica\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talvez o número de épocas tenha sido pouco para essa learning rate, visto que ela foi aumentando a acuracia aos poucos, vamos então aumentar para 60 épocas e adicionar um dropout para evitar overfiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.7799 - loss: 0.4742 - val_accuracy: 0.8826 - val_loss: 0.2988\n",
      "Epoch 2/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8826 - loss: 0.2944 - val_accuracy: 0.9047 - val_loss: 0.2392\n",
      "Epoch 3/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9043 - loss: 0.2438 - val_accuracy: 0.9158 - val_loss: 0.2114\n",
      "Epoch 4/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9136 - loss: 0.2173 - val_accuracy: 0.9266 - val_loss: 0.1900\n",
      "Epoch 5/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9174 - loss: 0.2056 - val_accuracy: 0.9298 - val_loss: 0.1781\n",
      "Epoch 6/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9237 - loss: 0.1909 - val_accuracy: 0.9320 - val_loss: 0.1682\n",
      "Epoch 7/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9279 - loss: 0.1783 - val_accuracy: 0.9325 - val_loss: 0.1623\n",
      "Epoch 8/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9295 - loss: 0.1714 - val_accuracy: 0.9354 - val_loss: 0.1574\n",
      "Epoch 9/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9315 - loss: 0.1652 - val_accuracy: 0.9382 - val_loss: 0.1501\n",
      "Epoch 10/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 996us/step - accuracy: 0.9319 - loss: 0.1639 - val_accuracy: 0.9388 - val_loss: 0.1455\n",
      "Epoch 11/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9360 - loss: 0.1580 - val_accuracy: 0.9394 - val_loss: 0.1458\n",
      "Epoch 12/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9366 - loss: 0.1545 - val_accuracy: 0.9401 - val_loss: 0.1422\n",
      "Epoch 13/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9382 - loss: 0.1495 - val_accuracy: 0.9414 - val_loss: 0.1380\n",
      "Epoch 14/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9385 - loss: 0.1468 - val_accuracy: 0.9432 - val_loss: 0.1367\n",
      "Epoch 15/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9404 - loss: 0.1457 - val_accuracy: 0.9427 - val_loss: 0.1358\n",
      "Epoch 16/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9416 - loss: 0.1444 - val_accuracy: 0.9435 - val_loss: 0.1341\n",
      "Epoch 17/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9398 - loss: 0.1433 - val_accuracy: 0.9440 - val_loss: 0.1323\n",
      "Epoch 18/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9400 - loss: 0.1423 - val_accuracy: 0.9447 - val_loss: 0.1302\n",
      "Epoch 19/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9407 - loss: 0.1403 - val_accuracy: 0.9451 - val_loss: 0.1326\n",
      "Epoch 20/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9428 - loss: 0.1386 - val_accuracy: 0.9455 - val_loss: 0.1295\n",
      "Epoch 21/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9427 - loss: 0.1391 - val_accuracy: 0.9458 - val_loss: 0.1290\n",
      "Epoch 22/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9412 - loss: 0.1395 - val_accuracy: 0.9452 - val_loss: 0.1295\n",
      "Epoch 23/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9409 - loss: 0.1386 - val_accuracy: 0.9460 - val_loss: 0.1263\n",
      "Epoch 24/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9415 - loss: 0.1384 - val_accuracy: 0.9467 - val_loss: 0.1269\n",
      "Epoch 25/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9440 - loss: 0.1342 - val_accuracy: 0.9446 - val_loss: 0.1281\n",
      "Epoch 26/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9439 - loss: 0.1360 - val_accuracy: 0.9467 - val_loss: 0.1275\n",
      "Epoch 27/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9441 - loss: 0.1348 - val_accuracy: 0.9470 - val_loss: 0.1242\n",
      "Epoch 28/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9437 - loss: 0.1342 - val_accuracy: 0.9472 - val_loss: 0.1252\n",
      "Epoch 29/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9442 - loss: 0.1327 - val_accuracy: 0.9480 - val_loss: 0.1231\n",
      "Epoch 30/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9449 - loss: 0.1312 - val_accuracy: 0.9483 - val_loss: 0.1226\n",
      "Epoch 31/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9452 - loss: 0.1289 - val_accuracy: 0.9481 - val_loss: 0.1226\n",
      "Epoch 32/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9431 - loss: 0.1338 - val_accuracy: 0.9489 - val_loss: 0.1220\n",
      "Epoch 33/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9450 - loss: 0.1313 - val_accuracy: 0.9486 - val_loss: 0.1218\n",
      "Epoch 34/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9466 - loss: 0.1287 - val_accuracy: 0.9498 - val_loss: 0.1208\n",
      "Epoch 35/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9458 - loss: 0.1288 - val_accuracy: 0.9469 - val_loss: 0.1235\n",
      "Epoch 36/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9465 - loss: 0.1287 - val_accuracy: 0.9494 - val_loss: 0.1220\n",
      "Epoch 37/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9459 - loss: 0.1298 - val_accuracy: 0.9478 - val_loss: 0.1216\n",
      "Epoch 38/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9461 - loss: 0.1288 - val_accuracy: 0.9471 - val_loss: 0.1225\n",
      "Epoch 39/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9467 - loss: 0.1270 - val_accuracy: 0.9492 - val_loss: 0.1193\n",
      "Epoch 40/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9475 - loss: 0.1261 - val_accuracy: 0.9490 - val_loss: 0.1195\n",
      "Epoch 41/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9473 - loss: 0.1250 - val_accuracy: 0.9488 - val_loss: 0.1191\n",
      "Epoch 42/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 985us/step - accuracy: 0.9451 - loss: 0.1292 - val_accuracy: 0.9501 - val_loss: 0.1191\n",
      "Epoch 43/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9462 - loss: 0.1257 - val_accuracy: 0.9478 - val_loss: 0.1203\n",
      "Epoch 44/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9462 - loss: 0.1271 - val_accuracy: 0.9440 - val_loss: 0.1222\n",
      "Epoch 45/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9463 - loss: 0.1250 - val_accuracy: 0.9459 - val_loss: 0.1214\n",
      "Epoch 46/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9458 - loss: 0.1271 - val_accuracy: 0.9469 - val_loss: 0.1208\n",
      "Epoch 47/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9481 - loss: 0.1247 - val_accuracy: 0.9485 - val_loss: 0.1193\n",
      "Epoch 48/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9470 - loss: 0.1246 - val_accuracy: 0.9494 - val_loss: 0.1181\n",
      "Epoch 49/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9459 - loss: 0.1265 - val_accuracy: 0.9493 - val_loss: 0.1173\n",
      "Epoch 50/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9462 - loss: 0.1261 - val_accuracy: 0.9499 - val_loss: 0.1179\n",
      "Epoch 51/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9471 - loss: 0.1260 - val_accuracy: 0.9498 - val_loss: 0.1176\n",
      "Epoch 52/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9474 - loss: 0.1243 - val_accuracy: 0.9470 - val_loss: 0.1202\n",
      "Epoch 53/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9463 - loss: 0.1294 - val_accuracy: 0.9507 - val_loss: 0.1172\n",
      "Epoch 54/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 990us/step - accuracy: 0.9484 - loss: 0.1247 - val_accuracy: 0.9510 - val_loss: 0.1180\n",
      "Epoch 55/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9466 - loss: 0.1289 - val_accuracy: 0.9476 - val_loss: 0.1216\n",
      "Epoch 56/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9482 - loss: 0.1229 - val_accuracy: 0.9496 - val_loss: 0.1186\n",
      "Epoch 57/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9482 - loss: 0.1235 - val_accuracy: 0.9492 - val_loss: 0.1165\n",
      "Epoch 58/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9476 - loss: 0.1243 - val_accuracy: 0.9492 - val_loss: 0.1173\n",
      "Epoch 59/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9470 - loss: 0.1254 - val_accuracy: 0.9499 - val_loss: 0.1169\n",
      "Epoch 60/60\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9488 - loss: 0.1227 - val_accuracy: 0.9458 - val_loss: 0.1277\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 644us/step\n",
      "Acurácia: 0.9496458269171543\n"
     ]
    }
   ],
   "source": [
    "# Definindo um otimizador com learning rate específica\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=60, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos aumentar ainda mais a acuracia final, porém desde a época 13 ate à época 60 a diferença foi muito pouca, vou tentar então voltar ao número anterior de learning rate e agora vou tentar adicionar outra camada oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.8541 - loss: 0.3384 - val_accuracy: 0.9292 - val_loss: 0.1730\n",
      "Epoch 2/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9291 - loss: 0.1721 - val_accuracy: 0.9332 - val_loss: 0.1565\n",
      "Epoch 3/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9357 - loss: 0.1513 - val_accuracy: 0.9401 - val_loss: 0.1435\n",
      "Epoch 4/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9389 - loss: 0.1426 - val_accuracy: 0.9424 - val_loss: 0.1384\n",
      "Epoch 5/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9416 - loss: 0.1361 - val_accuracy: 0.9358 - val_loss: 0.1358\n",
      "Epoch 6/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9429 - loss: 0.1315 - val_accuracy: 0.9461 - val_loss: 0.1244\n",
      "Epoch 7/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9463 - loss: 0.1258 - val_accuracy: 0.9466 - val_loss: 0.1239\n",
      "Epoch 8/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9452 - loss: 0.1236 - val_accuracy: 0.9427 - val_loss: 0.1256\n",
      "Epoch 9/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9474 - loss: 0.1204 - val_accuracy: 0.9361 - val_loss: 0.1339\n",
      "Epoch 10/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9480 - loss: 0.1188 - val_accuracy: 0.9487 - val_loss: 0.1166\n",
      "Epoch 11/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9485 - loss: 0.1159 - val_accuracy: 0.9451 - val_loss: 0.1246\n",
      "Epoch 12/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9493 - loss: 0.1146 - val_accuracy: 0.9458 - val_loss: 0.1233\n",
      "Epoch 13/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9512 - loss: 0.1123 - val_accuracy: 0.9443 - val_loss: 0.1179\n",
      "Epoch 14/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9518 - loss: 0.1110 - val_accuracy: 0.9504 - val_loss: 0.1115\n",
      "Epoch 15/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9513 - loss: 0.1097 - val_accuracy: 0.9503 - val_loss: 0.1143\n",
      "Epoch 16/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9529 - loss: 0.1087 - val_accuracy: 0.9490 - val_loss: 0.1144\n",
      "Epoch 17/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9522 - loss: 0.1093 - val_accuracy: 0.9414 - val_loss: 0.1252\n",
      "Epoch 18/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9525 - loss: 0.1074 - val_accuracy: 0.9507 - val_loss: 0.1094\n",
      "Epoch 19/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9524 - loss: 0.1084 - val_accuracy: 0.9488 - val_loss: 0.1171\n",
      "Epoch 20/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9537 - loss: 0.1058 - val_accuracy: 0.9512 - val_loss: 0.1096\n",
      "\u001b[1m3680/3680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 774us/step\n",
      "Acurácia: 0.955332121809134\n"
     ]
    }
   ],
   "source": [
    "# Definindo um otimizador com learning rate específica\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=32, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos o melhor resultado até agora, vamos tentar adicionar mais uma camada oculta para ver se mantemos a melhoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.8680 - loss: 0.3036 - val_accuracy: 0.9234 - val_loss: 0.1683\n",
      "Epoch 2/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9344 - loss: 0.1541 - val_accuracy: 0.9398 - val_loss: 0.1445\n",
      "Epoch 3/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9388 - loss: 0.1406 - val_accuracy: 0.9421 - val_loss: 0.1255\n",
      "Epoch 4/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9418 - loss: 0.1332 - val_accuracy: 0.9354 - val_loss: 0.1339\n",
      "Epoch 5/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9451 - loss: 0.1240 - val_accuracy: 0.9455 - val_loss: 0.1188\n",
      "Epoch 6/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9474 - loss: 0.1188 - val_accuracy: 0.9495 - val_loss: 0.1168\n",
      "Epoch 7/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9493 - loss: 0.1153 - val_accuracy: 0.9511 - val_loss: 0.1123\n",
      "Epoch 8/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9502 - loss: 0.1132 - val_accuracy: 0.9511 - val_loss: 0.1122\n",
      "Epoch 9/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9522 - loss: 0.1082 - val_accuracy: 0.9493 - val_loss: 0.1190\n",
      "Epoch 10/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9529 - loss: 0.1061 - val_accuracy: 0.9524 - val_loss: 0.1053\n",
      "Epoch 11/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9545 - loss: 0.1030 - val_accuracy: 0.9450 - val_loss: 0.1290\n",
      "Epoch 12/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9556 - loss: 0.1024 - val_accuracy: 0.9515 - val_loss: 0.1084\n",
      "Epoch 13/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9552 - loss: 0.0998 - val_accuracy: 0.9552 - val_loss: 0.1003\n",
      "Epoch 14/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9573 - loss: 0.0978 - val_accuracy: 0.9510 - val_loss: 0.1093\n",
      "Epoch 15/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9583 - loss: 0.0959 - val_accuracy: 0.9539 - val_loss: 0.1025\n",
      "Epoch 16/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9578 - loss: 0.0952 - val_accuracy: 0.9542 - val_loss: 0.1031\n",
      "Epoch 17/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9572 - loss: 0.0964 - val_accuracy: 0.9497 - val_loss: 0.1211\n",
      "Epoch 18/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9585 - loss: 0.0932 - val_accuracy: 0.9551 - val_loss: 0.1020\n",
      "Epoch 19/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9582 - loss: 0.0936 - val_accuracy: 0.9548 - val_loss: 0.1031\n",
      "Epoch 20/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9594 - loss: 0.0905 - val_accuracy: 0.9540 - val_loss: 0.1021\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 874us/step\n",
      "Acurácia: 0.9546889436402833\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não mantemos a melhoria anterior, mas vamos explorar mais camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.8704 - loss: 0.2969 - val_accuracy: 0.9332 - val_loss: 0.1574\n",
      "Epoch 2/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9339 - loss: 0.1572 - val_accuracy: 0.9412 - val_loss: 0.1393\n",
      "Epoch 3/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9393 - loss: 0.1402 - val_accuracy: 0.9411 - val_loss: 0.1369\n",
      "Epoch 4/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9430 - loss: 0.1312 - val_accuracy: 0.9479 - val_loss: 0.1218\n",
      "Epoch 5/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9462 - loss: 0.1226 - val_accuracy: 0.9482 - val_loss: 0.1161\n",
      "Epoch 6/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9483 - loss: 0.1168 - val_accuracy: 0.9504 - val_loss: 0.1161\n",
      "Epoch 7/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9503 - loss: 0.1150 - val_accuracy: 0.9466 - val_loss: 0.1196\n",
      "Epoch 8/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9523 - loss: 0.1100 - val_accuracy: 0.9499 - val_loss: 0.1145\n",
      "Epoch 9/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9544 - loss: 0.1054 - val_accuracy: 0.9514 - val_loss: 0.1076\n",
      "Epoch 10/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9543 - loss: 0.1036 - val_accuracy: 0.9532 - val_loss: 0.1048\n",
      "Epoch 11/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9553 - loss: 0.1021 - val_accuracy: 0.9531 - val_loss: 0.1057\n",
      "Epoch 12/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9560 - loss: 0.0993 - val_accuracy: 0.9537 - val_loss: 0.1031\n",
      "Epoch 13/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9563 - loss: 0.0987 - val_accuracy: 0.9499 - val_loss: 0.1133\n",
      "Epoch 14/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9564 - loss: 0.0995 - val_accuracy: 0.9538 - val_loss: 0.1028\n",
      "Epoch 15/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9567 - loss: 0.0978 - val_accuracy: 0.9538 - val_loss: 0.1066\n",
      "Epoch 16/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9582 - loss: 0.0949 - val_accuracy: 0.9554 - val_loss: 0.1010\n",
      "Epoch 17/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9580 - loss: 0.0953 - val_accuracy: 0.9533 - val_loss: 0.1028\n",
      "Epoch 18/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9591 - loss: 0.0921 - val_accuracy: 0.9545 - val_loss: 0.1012\n",
      "Epoch 19/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9592 - loss: 0.0929 - val_accuracy: 0.9564 - val_loss: 0.0985\n",
      "Epoch 20/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9598 - loss: 0.0897 - val_accuracy: 0.9570 - val_loss: 0.0979\n",
      "Epoch 21/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9605 - loss: 0.0885 - val_accuracy: 0.9573 - val_loss: 0.0981\n",
      "Epoch 22/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.9607 - loss: 0.0875 - val_accuracy: 0.9534 - val_loss: 0.1024\n",
      "Epoch 23/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9612 - loss: 0.0873 - val_accuracy: 0.9560 - val_loss: 0.1010\n",
      "Epoch 24/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9608 - loss: 0.0890 - val_accuracy: 0.9568 - val_loss: 0.0981\n",
      "Epoch 25/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9617 - loss: 0.0864 - val_accuracy: 0.9532 - val_loss: 0.1054\n",
      "Epoch 26/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9614 - loss: 0.0851 - val_accuracy: 0.9562 - val_loss: 0.0988\n",
      "Epoch 27/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9629 - loss: 0.0829 - val_accuracy: 0.9557 - val_loss: 0.0996\n",
      "Epoch 28/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9625 - loss: 0.0835 - val_accuracy: 0.9566 - val_loss: 0.1022\n",
      "Epoch 29/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9634 - loss: 0.0806 - val_accuracy: 0.9546 - val_loss: 0.1070\n",
      "Epoch 30/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9620 - loss: 0.0838 - val_accuracy: 0.9558 - val_loss: 0.1033\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 895us/step\n",
      "Acurácia: 0.9574222359100708\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos chegar no melhore resultado, vamos tentar mantendo esse modelo, aumentar a época e diminuir a learning rate para tentar chegar aos 0.96 de acuracia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.8548 - loss: 0.3382 - val_accuracy: 0.9243 - val_loss: 0.1918\n",
      "Epoch 2/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9281 - loss: 0.1726 - val_accuracy: 0.9381 - val_loss: 0.1498\n",
      "Epoch 3/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9382 - loss: 0.1464 - val_accuracy: 0.9414 - val_loss: 0.1366\n",
      "Epoch 4/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9394 - loss: 0.1388 - val_accuracy: 0.9393 - val_loss: 0.1333\n",
      "Epoch 5/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9425 - loss: 0.1329 - val_accuracy: 0.9273 - val_loss: 0.1500\n",
      "Epoch 6/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9442 - loss: 0.1270 - val_accuracy: 0.9473 - val_loss: 0.1233\n",
      "Epoch 7/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9480 - loss: 0.1208 - val_accuracy: 0.9466 - val_loss: 0.1187\n",
      "Epoch 8/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9495 - loss: 0.1160 - val_accuracy: 0.9494 - val_loss: 0.1156\n",
      "Epoch 9/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9500 - loss: 0.1139 - val_accuracy: 0.9509 - val_loss: 0.1138\n",
      "Epoch 10/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9510 - loss: 0.1111 - val_accuracy: 0.9483 - val_loss: 0.1140\n",
      "Epoch 11/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9533 - loss: 0.1065 - val_accuracy: 0.9508 - val_loss: 0.1115\n",
      "Epoch 12/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9531 - loss: 0.1063 - val_accuracy: 0.9520 - val_loss: 0.1083\n",
      "Epoch 13/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9544 - loss: 0.1040 - val_accuracy: 0.9510 - val_loss: 0.1143\n",
      "Epoch 14/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9565 - loss: 0.1009 - val_accuracy: 0.9473 - val_loss: 0.1110\n",
      "Epoch 15/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9546 - loss: 0.1020 - val_accuracy: 0.9529 - val_loss: 0.1080\n",
      "Epoch 16/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9565 - loss: 0.0997 - val_accuracy: 0.9530 - val_loss: 0.1120\n",
      "Epoch 17/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9576 - loss: 0.0969 - val_accuracy: 0.9470 - val_loss: 0.1192\n",
      "Epoch 18/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9587 - loss: 0.0952 - val_accuracy: 0.9539 - val_loss: 0.1045\n",
      "Epoch 19/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9579 - loss: 0.0942 - val_accuracy: 0.9540 - val_loss: 0.1058\n",
      "Epoch 20/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9589 - loss: 0.0930 - val_accuracy: 0.9549 - val_loss: 0.1053\n",
      "Epoch 21/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9580 - loss: 0.0932 - val_accuracy: 0.9549 - val_loss: 0.1023\n",
      "Epoch 22/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9582 - loss: 0.0932 - val_accuracy: 0.9542 - val_loss: 0.1027\n",
      "Epoch 23/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9597 - loss: 0.0909 - val_accuracy: 0.9510 - val_loss: 0.1125\n",
      "Epoch 24/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9605 - loss: 0.0900 - val_accuracy: 0.9558 - val_loss: 0.1014\n",
      "Epoch 25/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9603 - loss: 0.0906 - val_accuracy: 0.9553 - val_loss: 0.1021\n",
      "Epoch 26/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9619 - loss: 0.0850 - val_accuracy: 0.9555 - val_loss: 0.1046\n",
      "Epoch 27/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9614 - loss: 0.0858 - val_accuracy: 0.9550 - val_loss: 0.1023\n",
      "Epoch 28/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9625 - loss: 0.0860 - val_accuracy: 0.9552 - val_loss: 0.1025\n",
      "Epoch 29/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9614 - loss: 0.0865 - val_accuracy: 0.9552 - val_loss: 0.1030\n",
      "Epoch 30/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9633 - loss: 0.0835 - val_accuracy: 0.9545 - val_loss: 0.1025\n",
      "Epoch 31/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9627 - loss: 0.0840 - val_accuracy: 0.9520 - val_loss: 0.1039\n",
      "Epoch 32/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9635 - loss: 0.0818 - val_accuracy: 0.9531 - val_loss: 0.1061\n",
      "Epoch 33/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9631 - loss: 0.0822 - val_accuracy: 0.9559 - val_loss: 0.1018\n",
      "Epoch 34/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9638 - loss: 0.0827 - val_accuracy: 0.9558 - val_loss: 0.1000\n",
      "Epoch 35/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9636 - loss: 0.0810 - val_accuracy: 0.9524 - val_loss: 0.1087\n",
      "Epoch 36/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9645 - loss: 0.0802 - val_accuracy: 0.9564 - val_loss: 0.1053\n",
      "Epoch 37/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9639 - loss: 0.0800 - val_accuracy: 0.9571 - val_loss: 0.1028\n",
      "Epoch 38/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9651 - loss: 0.0780 - val_accuracy: 0.9542 - val_loss: 0.1064\n",
      "Epoch 39/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9645 - loss: 0.0794 - val_accuracy: 0.9564 - val_loss: 0.1049\n",
      "Epoch 40/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9668 - loss: 0.0751 - val_accuracy: 0.9560 - val_loss: 0.1036\n",
      "Epoch 41/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9655 - loss: 0.0766 - val_accuracy: 0.9555 - val_loss: 0.1023\n",
      "Epoch 42/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9664 - loss: 0.0765 - val_accuracy: 0.9567 - val_loss: 0.1037\n",
      "Epoch 43/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9656 - loss: 0.0763 - val_accuracy: 0.9549 - val_loss: 0.1064\n",
      "Epoch 44/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9666 - loss: 0.0738 - val_accuracy: 0.9566 - val_loss: 0.1036\n",
      "Epoch 45/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9656 - loss: 0.0751 - val_accuracy: 0.9552 - val_loss: 0.1087\n",
      "Epoch 46/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9669 - loss: 0.0736 - val_accuracy: 0.9547 - val_loss: 0.1092\n",
      "Epoch 47/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9667 - loss: 0.0742 - val_accuracy: 0.9580 - val_loss: 0.1043\n",
      "Epoch 48/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9676 - loss: 0.0716 - val_accuracy: 0.9522 - val_loss: 0.1118\n",
      "Epoch 49/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9676 - loss: 0.0716 - val_accuracy: 0.9573 - val_loss: 0.1062\n",
      "Epoch 50/50\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9680 - loss: 0.0708 - val_accuracy: 0.9564 - val_loss: 0.1092\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 840us/step\n",
      "Acurácia: 0.9538420080073914\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar de conseguir 0.9680 na ultima época a acuracia final foi ruim, vamos testar esse mesmo exemplo com menos épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8679 - loss: 0.3193 - val_accuracy: 0.9281 - val_loss: 0.1676\n",
      "Epoch 2/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9318 - loss: 0.1679 - val_accuracy: 0.9378 - val_loss: 0.1515\n",
      "Epoch 3/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9371 - loss: 0.1488 - val_accuracy: 0.9408 - val_loss: 0.1338\n",
      "Epoch 4/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9406 - loss: 0.1377 - val_accuracy: 0.9415 - val_loss: 0.1326\n",
      "Epoch 5/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9425 - loss: 0.1325 - val_accuracy: 0.9433 - val_loss: 0.1322\n",
      "Epoch 6/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9454 - loss: 0.1255 - val_accuracy: 0.9287 - val_loss: 0.1486\n",
      "Epoch 7/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9483 - loss: 0.1190 - val_accuracy: 0.9472 - val_loss: 0.1178\n",
      "Epoch 8/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9487 - loss: 0.1165 - val_accuracy: 0.9490 - val_loss: 0.1154\n",
      "Epoch 9/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9498 - loss: 0.1146 - val_accuracy: 0.9461 - val_loss: 0.1208\n",
      "Epoch 10/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9511 - loss: 0.1105 - val_accuracy: 0.9493 - val_loss: 0.1116\n",
      "Epoch 11/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9537 - loss: 0.1062 - val_accuracy: 0.9511 - val_loss: 0.1097\n",
      "Epoch 12/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9543 - loss: 0.1041 - val_accuracy: 0.9441 - val_loss: 0.1192\n",
      "Epoch 13/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9531 - loss: 0.1050 - val_accuracy: 0.9512 - val_loss: 0.1112\n",
      "Epoch 14/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9539 - loss: 0.1042 - val_accuracy: 0.9522 - val_loss: 0.1104\n",
      "Epoch 15/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9562 - loss: 0.1009 - val_accuracy: 0.9507 - val_loss: 0.1125\n",
      "Epoch 16/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9567 - loss: 0.0979 - val_accuracy: 0.9538 - val_loss: 0.1076\n",
      "Epoch 17/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9571 - loss: 0.0970 - val_accuracy: 0.9545 - val_loss: 0.1082\n",
      "Epoch 18/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9572 - loss: 0.0952 - val_accuracy: 0.9555 - val_loss: 0.1015\n",
      "Epoch 19/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9587 - loss: 0.0933 - val_accuracy: 0.9518 - val_loss: 0.1044\n",
      "Epoch 20/20\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9581 - loss: 0.0937 - val_accuracy: 0.9547 - val_loss: 0.1014\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 817us/step\n",
      "Acurácia: 0.9537650138589467\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda não conseguimos bater a learning rate de 0.001, vamos fazer o ultimo teste com 0.0005 mas agora com 30 épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.8579 - loss: 0.3466 - val_accuracy: 0.9171 - val_loss: 0.2072\n",
      "Epoch 2/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9284 - loss: 0.1730 - val_accuracy: 0.9309 - val_loss: 0.1548\n",
      "Epoch 3/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9354 - loss: 0.1509 - val_accuracy: 0.9322 - val_loss: 0.1568\n",
      "Epoch 4/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9424 - loss: 0.1358 - val_accuracy: 0.9431 - val_loss: 0.1346\n",
      "Epoch 5/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9432 - loss: 0.1302 - val_accuracy: 0.9378 - val_loss: 0.1336\n",
      "Epoch 6/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9472 - loss: 0.1219 - val_accuracy: 0.9468 - val_loss: 0.1206\n",
      "Epoch 7/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9487 - loss: 0.1189 - val_accuracy: 0.9340 - val_loss: 0.1441\n",
      "Epoch 8/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9484 - loss: 0.1177 - val_accuracy: 0.9478 - val_loss: 0.1216\n",
      "Epoch 9/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9514 - loss: 0.1099 - val_accuracy: 0.9484 - val_loss: 0.1158\n",
      "Epoch 10/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9509 - loss: 0.1113 - val_accuracy: 0.9508 - val_loss: 0.1114\n",
      "Epoch 11/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9514 - loss: 0.1084 - val_accuracy: 0.9507 - val_loss: 0.1145\n",
      "Epoch 12/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9540 - loss: 0.1037 - val_accuracy: 0.9486 - val_loss: 0.1134\n",
      "Epoch 13/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9538 - loss: 0.1024 - val_accuracy: 0.9482 - val_loss: 0.1129\n",
      "Epoch 14/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9546 - loss: 0.1015 - val_accuracy: 0.9507 - val_loss: 0.1113\n",
      "Epoch 15/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9556 - loss: 0.0998 - val_accuracy: 0.9535 - val_loss: 0.1070\n",
      "Epoch 16/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9558 - loss: 0.0992 - val_accuracy: 0.9519 - val_loss: 0.1067\n",
      "Epoch 17/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9583 - loss: 0.0957 - val_accuracy: 0.9513 - val_loss: 0.1067\n",
      "Epoch 18/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9557 - loss: 0.0976 - val_accuracy: 0.9540 - val_loss: 0.1048\n",
      "Epoch 19/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9594 - loss: 0.0922 - val_accuracy: 0.9518 - val_loss: 0.1102\n",
      "Epoch 20/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9580 - loss: 0.0940 - val_accuracy: 0.9529 - val_loss: 0.1079\n",
      "Epoch 21/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9603 - loss: 0.0919 - val_accuracy: 0.9518 - val_loss: 0.1110\n",
      "Epoch 22/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9606 - loss: 0.0893 - val_accuracy: 0.9539 - val_loss: 0.1098\n",
      "Epoch 23/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9607 - loss: 0.0890 - val_accuracy: 0.9546 - val_loss: 0.1079\n",
      "Epoch 24/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9613 - loss: 0.0875 - val_accuracy: 0.9522 - val_loss: 0.1029\n",
      "Epoch 25/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9621 - loss: 0.0860 - val_accuracy: 0.9540 - val_loss: 0.1090\n",
      "Epoch 26/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9616 - loss: 0.0859 - val_accuracy: 0.9539 - val_loss: 0.1059\n",
      "Epoch 27/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9625 - loss: 0.0851 - val_accuracy: 0.9543 - val_loss: 0.1083\n",
      "Epoch 28/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9628 - loss: 0.0838 - val_accuracy: 0.9558 - val_loss: 0.1034\n",
      "Epoch 29/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9612 - loss: 0.0850 - val_accuracy: 0.9551 - val_loss: 0.1033\n",
      "Epoch 30/30\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9623 - loss: 0.0834 - val_accuracy: 0.9528 - val_loss: 0.1055\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step\n",
      "Acurácia: 0.9511857098860487\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais uma vez durante o treinamento conseguimos acuracia boa mas a final não é satisfatoria, vamos fazer o ultimo teste com 0.0005 de learning rate e qualquer coisa voltamos para 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luize\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.8521 - loss: 0.3284 - val_accuracy: 0.9180 - val_loss: 0.1878\n",
      "Epoch 2/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9312 - loss: 0.1658 - val_accuracy: 0.9312 - val_loss: 0.1567\n",
      "Epoch 3/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9380 - loss: 0.1451 - val_accuracy: 0.9417 - val_loss: 0.1401\n",
      "Epoch 4/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9412 - loss: 0.1358 - val_accuracy: 0.9430 - val_loss: 0.1299\n",
      "Epoch 5/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9458 - loss: 0.1267 - val_accuracy: 0.9416 - val_loss: 0.1385\n",
      "Epoch 6/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9453 - loss: 0.1234 - val_accuracy: 0.9484 - val_loss: 0.1200\n",
      "Epoch 7/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9487 - loss: 0.1182 - val_accuracy: 0.9485 - val_loss: 0.1202\n",
      "Epoch 8/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9490 - loss: 0.1156 - val_accuracy: 0.9476 - val_loss: 0.1218\n",
      "Epoch 9/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9509 - loss: 0.1106 - val_accuracy: 0.9514 - val_loss: 0.1106\n",
      "Epoch 10/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9519 - loss: 0.1082 - val_accuracy: 0.9501 - val_loss: 0.1159\n",
      "Epoch 11/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9546 - loss: 0.1036 - val_accuracy: 0.9529 - val_loss: 0.1099\n",
      "Epoch 12/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9546 - loss: 0.1036 - val_accuracy: 0.9501 - val_loss: 0.1137\n",
      "Epoch 13/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9567 - loss: 0.1005 - val_accuracy: 0.9505 - val_loss: 0.1136\n",
      "Epoch 14/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9554 - loss: 0.1015 - val_accuracy: 0.9515 - val_loss: 0.1099\n",
      "Epoch 15/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9560 - loss: 0.0995 - val_accuracy: 0.9492 - val_loss: 0.1104\n",
      "Epoch 16/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9566 - loss: 0.0978 - val_accuracy: 0.9529 - val_loss: 0.1041\n",
      "Epoch 17/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9588 - loss: 0.0936 - val_accuracy: 0.9542 - val_loss: 0.1041\n",
      "Epoch 18/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9585 - loss: 0.0935 - val_accuracy: 0.9521 - val_loss: 0.1162\n",
      "Epoch 19/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9566 - loss: 0.0957 - val_accuracy: 0.9531 - val_loss: 0.1102\n",
      "Epoch 20/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9593 - loss: 0.0929 - val_accuracy: 0.9527 - val_loss: 0.1061\n",
      "Epoch 21/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9585 - loss: 0.0925 - val_accuracy: 0.9556 - val_loss: 0.1015\n",
      "Epoch 22/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9596 - loss: 0.0901 - val_accuracy: 0.9526 - val_loss: 0.1076\n",
      "Epoch 23/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9605 - loss: 0.0884 - val_accuracy: 0.9536 - val_loss: 0.1082\n",
      "Epoch 24/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9594 - loss: 0.0885 - val_accuracy: 0.9554 - val_loss: 0.1023\n",
      "Epoch 25/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9609 - loss: 0.0874 - val_accuracy: 0.9540 - val_loss: 0.1048\n",
      "Epoch 26/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9618 - loss: 0.0850 - val_accuracy: 0.9553 - val_loss: 0.1021\n",
      "Epoch 27/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9620 - loss: 0.0853 - val_accuracy: 0.9547 - val_loss: 0.1051\n",
      "Epoch 28/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9624 - loss: 0.0852 - val_accuracy: 0.9542 - val_loss: 0.1022\n",
      "Epoch 29/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9628 - loss: 0.0834 - val_accuracy: 0.9557 - val_loss: 0.1015\n",
      "Epoch 30/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9635 - loss: 0.0826 - val_accuracy: 0.9542 - val_loss: 0.1078\n",
      "Epoch 31/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9627 - loss: 0.0829 - val_accuracy: 0.9550 - val_loss: 0.1073\n",
      "Epoch 32/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9641 - loss: 0.0814 - val_accuracy: 0.9531 - val_loss: 0.1066\n",
      "Epoch 33/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9636 - loss: 0.0808 - val_accuracy: 0.9557 - val_loss: 0.1047\n",
      "Epoch 34/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9638 - loss: 0.0797 - val_accuracy: 0.9563 - val_loss: 0.1021\n",
      "Epoch 35/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9636 - loss: 0.0800 - val_accuracy: 0.9558 - val_loss: 0.1034\n",
      "Epoch 36/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9645 - loss: 0.0798 - val_accuracy: 0.9549 - val_loss: 0.1020\n",
      "Epoch 37/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9646 - loss: 0.0771 - val_accuracy: 0.9563 - val_loss: 0.1014\n",
      "Epoch 38/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9649 - loss: 0.0772 - val_accuracy: 0.9550 - val_loss: 0.1021\n",
      "Epoch 39/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9663 - loss: 0.0750 - val_accuracy: 0.9538 - val_loss: 0.1165\n",
      "Epoch 40/40\n",
      "\u001b[1m2760/2760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9656 - loss: 0.0761 - val_accuracy: 0.9558 - val_loss: 0.1077\n",
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 767us/step\n",
      "Acurácia: 0.9560363412380659\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo com o otimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Avaliando o modelo\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "print(f'Acurácia: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chegamos no nosso melhor resultado, apesar de o treinamento mostrar varias epocas com a acuracia superior a .96, conseguimos no final .956, que também está de bom tamanho, agora vamos plotar uns gráficos para melhor entendimento dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 820us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAGDCAYAAAALVDiWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAAsTAAALEwEAmpwYAAAg90lEQVR4nO3dd5hU5dmA8ftZlt6bigZUVCRGxRhb7B1sMbbY8sUYDdbEL0aj0UQjlmCN3dgLWCIqiV0/W4zGAgoSwA4qikqXjpT3+2OGdReXZSE77Avcv+viunbOOXPmmVW495w5MxspJSRJUr7K6nsASZJUM2MtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLW0goiIphHxSER8FRED/ov9HBkRT9flbPUhIp6IiKPqew5peTDWUh2LiCMiYnBETI+Iz4tR2b4Odn0wsDrQPqV0yLLuJKV0d0ppzzqYp4qI2DkiUkQMXGR5j+LyF2q5nz9FRP8lbZdS2iuldOcyjiutUIy1VIci4lTgSuAiCmHtAlwP7F8Hu18beC+lNK8O9lUq44EfRkT7SsuOAt6rqweIAv/t0irF/+GlOhIRrYE+wEkppYdSSjNSSnNTSo+klE4vbtM4Iq6MiLHFP1dGROPiup0j4tOI+G1EjCselR9dXHcecA5waPGI/ZhFj0AjYp3iEWx58fbPI2JUREyLiNERcWSl5S9Vut+2ETGoeHp9UERsW2ndCxFxfkS8XNzP0xHRoYZvw9fA34HDivdvABwK3L3I9+qqiBgTEVMj4o2I2KG4vBdwVqXn+ValOS6MiJeBmUDX4rJji+tviIgHK+3/4oh4NiKitv/9pJwZa6nu/BBoAgysYZuzgW2AzYAewFbAHyqtXwNoDawFHANcFxFtU0rnUjha/1tKqUVK6daaBomI5sDVwF4ppZbAtsDQarZrBzxW3LY9cAXw2CJHxkcARwOrAY2A02p6bOAu4GfFr3sCw4Gxi2wziML3oB1wDzAgIpqklJ5c5Hn2qHSf/wF6Ay2BjxfZ32+BTYo/iOxA4Xt3VPLzlLWSMNZS3WkPTFjCaeojgT4ppXEppfHAeRQitNDc4vq5KaXHgenAhss4zwJg44homlL6PKU0oppt9gHeTyn1SynNSyndC7wD7Fdpm9tTSu+llGYB91OI7GKllP4NtIuIDSlE+65qtumfUppYfMzLgcYs+XnekVIaUbzP3EX2N5PC9/EKoD/wq5TSp0vYn7TCMNZS3ZkIdFh4Gnox1qTqUeHHxWUV+1gk9jOBFks7SEppBoXTz8cDn0fEYxHRvRbzLJxprUq3v1iGefoBJwO7UM2Zhog4LSLeLp56n0LhbEJNp9cBxtS0MqX0GjAKCAo/VEgrDWMt1Z1XgDnAj2vYZiyFC8UW6sK3TxHX1gygWaXba1RemVJ6KqW0B9CJwtHyzbWYZ+FMny3jTAv1A04EHi8e9VYonqb+HfAToG1KqQ3wFYXIAizu1HWNp7Qj4iQKR+hji/uXVhrGWqojKaWvKFwEdl1E/DgimkVEw4jYKyIuKW52L/CHiOhYvFDrHAqnbZfFUGDHiOhSvLjt9wtXRMTqEbF/8bXrORROpy+oZh+PA92Kbzcrj4hDgY2AR5dxJgBSSqOBnSi8Rr+olsA8CleOl0fEOUCrSuu/BNZZmiu+I6IbcAHwUwqnw38XEZst2/RSfoy1VIeKr7+eSuGisfEUTt2eTOEKaSgEZTAwDPgP8GZx2bI81v8Bfyvu6w2qBrasOMdYYBKFcJ5QzT4mAvtSuEBrIoUj0n1TShOWZaZF9v1SSqm6swZPAU9SeDvXx8Bsqp7iXviBLxMj4s0lPU7xZYf+wMUppbdSSu9TuKK838Ir7aUVXXixpCRJefPIWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzNX0SUv1zcvUJUmrmmp/+UzOsabp90+u7xGkVc6sIdcyO+dfwimtxJospsqeBpckKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrsd3m6zHgyuP48KkLmDXkWn6639aL3faasw9j1pBr+d//2a3K8l8cuB1P3vRrPn/xEmYNuZYundotdh+NG5Xz2t/OZNaQa9l8oy5V1s0acu23/hx78Pb/3ROUVhK33nwjPb63IRdd0AeAuXPn8pfLL+XgA/Zj6y02Y7edtufM03/L52PHVrnfhPHjOevM09l1x+3Y+gc9OOSAH/HYow/Xx1PQMiqv7wFU/1o0a8zID8Zyz6OvcUufny12uwN234wtNl6bseOmfGtdsyYNeebVd3j0hWFcevrBNT5e398cwGdfTmHTbt+pdv0Jfe7miReHV9z+avrs2j0RaSU27K2hPDDgb3TrtmHFstmzZ/PO2yM5tvcJdO/enWnTpnP5pX058bhjGTDwYcrLC//En33WGUz9agpXXns97dq249ln/4+zz/wda6zRiR9ssWV9PSUtBY+sxVMvjeTcax9h4DNDWZBStdt06dSWy04/mJ+fdQdz583/1vpr73mBy257mn8PHVXjY+278ybsuGU3fv+XgYvd5qtps/hy4rSKP7PnzF26JyStZKZNm8bvzziN886/iFatW1csb9myJTfecju99tqbddbtyiabbsofz+3DqFEfMnrUhxXbvTVkCIcefiSbbtqD73TuzFE//wVrrNGJ4f8ZVh9PR8vAWGuJGjQo484/H03fW57k3dFfLvN+1lqtDVefdRg/P+sOZs1efIAvO/1gxjzXl5f6n86xB29PRCzzY0orgz5/+iO779GTrbbeZonbzpgxHYBWrb6J+vc335ynn3yCKVMms2DBAp5/7hkmT57E1ttsW7KZVbdKdho8IroD+wNrFRd9BjycUnq7VI+p0vjj8fswYcoMbh7w0jLvo6wsuP2io7iq37P8573PFvua9nnXP8qLg95j+sw57LL1hvQ99QDat2nOxbc8tcyPLa3IHhxwP2M++YSL+l66xG3nfv01l1/Sl5123oXV11ijYvmlV1zFGaf9hp2224by8nIaNmxE30svp/t3v1vK0VWHShLriDgDOBy4D3i9uPg7wL0RcV9Kqe9i7tcb6A1w4403lmI0LaUdfrAB//Ojrdn6sGr/k9Xa747pyddz53NVv+dq3K7vzU9WfD3svc9oUFbG747paay1Svpo9CiuueoK7uh3Dw0bNqxx23nz5nHWmaczddo0rrruhirrrr36SiZPnsxNt95BmzZtef65Z/jD78/gtjvvZsPu3Uv5FFRHSnVkfQzwvZRSlXOdEXEFMAKo9l/+lNJNwE0Lb55yw8klGk+1teMWG7BGh1aMfvrCimXl5Q244JT9OfnInVm/1x9rtZ9dttqQ7b6/HtMGXVVl+T/v/C0PPP0mR599Z7X3e334R7Ru2ZTV2rVk3KRpy/5EpBXQW0OHMnnyZA7cf9+KZfPnz+eNwYN44P77eHXwUBo1asS8efM48/RTef/997j19n60adO2Yvsxn3zCvXf34/4H/1ER5g27d+fNNwZz7z39+FOfC7/1uMpPqWK9AFgT+HiR5Z2K67SCuOn+Fxn4zJAqyx65/iTuf/INbnvo5Vrvp/e5/WnetFHF7U4dW/PoDSdz9Nl38koNF6X16PYdZs3+minTZi398NIKbpfddueBjTeusuzcs39Pl7XX4Zjex9GwYUPmzp3LGaedygcfvMetd/SjQ8eOVbafPbvwd6esQdVLlMrKGrBggf8cryhKFev/BZ6NiPeBMcVlXYD1AQ+XM9O8aSPW61z4C14WQedObdm021pMnjqTMV9MZvzk6VW2nztvPl9OmMr7H4+rWLZ6+5as3r4VG3RZDYDvrrcGbVo2ZcwXk5k8dSYfj51YZR/TZ84BYNSnE/is+FawvXfcmNXbt+K1YaOZNWcuO225AX88YR9ue+hlvp47r1RPX8pWq1ataNWqVZVlTZs1o1Xr1mywQTfmzZvH6aeewvDh/+Ga6/5KEEwYPx6AFi1b0qRJE9ZZtytduqzNReefx6mnnUGbNm147rlnePWVl7nymuvr42lpGZQk1imlJyOiG7AVVS8wG5RS+vb7flSvNt9obZ6+5ZSK2+ecsC/nnLAv/R5+ld7n9q/VPo49eAf+cPzeFbf/fs2JAPzynH70f+S1Wu1j7rz59P7JDlz82wMpKwtGfzqR8294jL/e/+JSPBtp1fHll1/w/HPPAnDYIQdWWdfngj+z/wEH0rBhQ679601cdcXl/Prk45k5cyZdOnehzwV/Zudddq2PsbUMIi3mfbUZSE2/70G4tLzNGnItsz2RIdWLJuVU+15V32ctSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZlbqlhHRNuI2LRUw0iSpG9bYqwj4oWIaBUR7YA3gZsj4orSjyZJkqB2R9atU0pTgQOBu1JKWwO7l3YsSZK0UG1iXR4RnYCfAI+WeB5JkrSI2sS6D/AU8EFKaVBEdAXeL+1YkiRpofIlbZBSGgAMqHR7FHBQKYeSJEnfWGysI+IaIC1ufUrp1yWZSJIkVVHTkfXg5TaFJElarMXGOqV0Z+XbEdEspTSz9CNJkqTKavM+6x9GxEjgneLtHhFxfcknkyRJQO2uBr8S6AlMBEgpvQXsWMKZJElSJbX6uNGU0phFFs0vwSySJKkaS3zrFjAmIrYFUkQ0BE4B3i7tWJIkaaHaHFkfD5wErAWMBTYr3pYkSctBbT4UZQJw5HKYRZIkVaM2V4N3jYhHImJ8RIyLiH8UP3JUkiQtB7U5DX4PcD/QCViTwkeP3lvKoSRJ0jdqE+tmKaV+KaV5xT/9gSalHkySJBXU9Nng7YpfPhERZwL3Ufis8EOBx5fDbJIkiZovMHuDQpyjePu4SusS8PtSDSVJkr5R02eDr7s8B5EkSdWrzYeiEBEbAxtR6bXqlNJdpRpKkiR9Y4mxjohzgZ0pxPpxYC/gJcBYS5K0HNTmavCDgd2AL1JKRwM9gNYlnUqSJFWoTaxnpZQWAPMiohUwDuhc2rEkSdJCtXnNenBEtAFupnCF+HTglVIOJUmSvhEppdpvHLEO0CqlNKxkE32j9oNJkrRyiGoXLi7WEbF5TXtLKb1ZB0PV+BDT59hraXlr0TjoeqqfeyTVh1FX7F1trGs6DX55DesSsOt/NZEkSaqVmj4UZZflOYgkSapeba4GlyRJ9chYS5KUOWMtSVLmlhjrKPhpRJxTvN0lIrYq/WiSJAlqd2R9PfBD4PDi7WnAdSWbSJIkVVGbTzDbOqW0eUQMAUgpTY6IRiWeS5IkFdXmyHpuRDSg+IliEdERWFDSqSRJUoXaxPpqYCCwWkRcSOHXY15U0qkkSVKFJZ4GTyndHRFvUPg1mQH8OKX0dsknkyRJQC1iHRFdgJnAI5WXpZQ+KeVgkiSpoDYXmD1G4fXqAJoA6wLvAt8r4VySJKmoNqfBN6l8u/jbuE4s2USSJKmKpf4Es+Kvxty6BLNIkqRq1OY161Mr3SwDNgfGlmwiSZJURW1es25Z6et5FF7DfrA040iSpEXVGOvih6G0TCmdtpzmkSRJi1jsa9YRUZ5Smg9stxznkSRJi6jpyPp1Cq9PD42Ih4EBwIyFK1NKD5V4NkmSRO1es24CTAR25Zv3WyfAWEuStBzUFOvVileCD+ebSC+USjqVJEmqUFOsGwAtqBrphYy1JEnLSU2x/jyl1Ge5TSJJkqpV0yeYVXdELUmSlrOaYr3bcptCkiQt1mJjnVKatDwHkSRJ1VvqX+QhSZKWL2MtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkrr+8BtGLYt9eufD527LeWb7fDTlx93Y28OXgQ/e68jbffHsH4ceM49/yL+NH+By52fxf2OYeHHrifU049nZ/9/JhSji5la8uubfnlzl3ZuHNr1mjdhNPvfYsHB31WZZtTem7AYdt0pnWzhgz9eArnPjiC97+cXmWbHTbswCk9N+C7a7Zi7vwFDP/0K356w+sV6zft3JrT99mQTTq3BmD4mK+45PF3GfbJVxXb7N1jDU7cfX3W7dicSdO/5q6XP+Lm50eX8NlraRhr1Uq/ex5g/oL5FbcnjB/PTw87iD327AXAzFkzWW/9Ddhnv/055w9n1rivZ55+khHD/0PH1VYr6cxS7po3Lue9L6YxcPBnXHZEj2+tP27Xrhyz07r87r5hjBo3nV/tuQF3Hb8Vu/f9JzPmFP4+7rHx6lxy2KZc/vi7nP7+MMoCNv5O64p9NGvUgNt7b8nzI8dx7kMjCOCkPdbnzt5bsf35zzFjznx26t6RK3+6GX0GjuSf74xn/dVbcNFPNmH23AX0e+nj5fXtUA08Da5aaduuHR06dKz48/K/XqR5ixbs0XMvALbfYSdOPuVUdt+zF2Wx+P+tPh/7GZddchEX9r2M8nJ/VtSq7YW3x3PZ4+/xxLAvWJDSt9YfveM6/PW5D3ly2Be898V0Trv3LZo3LudHm68JQFnAuQdsRN9H36H/vz9h9PgZfDhuBv9485uzYOut1oK2zRtx5VPvM2pcYf0VT7xH62YN6dqxBQAHbLEWz44cR/9/f8KYSbN4/u3x3PDshxy3S9fl843QEhlrLbWUEv8Y+AB777MfTZo0qfX95s2bx1ln/JZjfnkC63Zdr4QTSiu+zu2aslqrJrz07oSKZXPmLmDQqElsvk5bADbu3Jo12zZl7rwFPHzqdrz2p92487gt2WitVhX3GTV+OhOmzeEnW3emUYMyGjUo47BtOvPZpFm89+U0ABqVl/H13AVVHn/23Pms2bYpa7VtuhyerZbEWGupvfrKy3z22acccNBPlup+N15/DW3atOWQQw8v0WTSyqNjq8YATJg2p8ryCdPm0LFlYV2Xds0A+E2vbtzwzIccc8sgvpgym3tP3Lpimxlz5nPE9a+xT49OjLi4JyMu7sk+m3XiZze+zpxioF98Zzy7b7w623frQASs27E5x+5cOKperTiH6tdyj3VEHF3Dut4RMTgiBt90003LcywthYEPDuB7G29Ctw271/o+gwe9xiMPD+Sc8y4s4WTSqqWsLAC47pkPeGLYFwz/dCpnDRjOtNnzOHDLtQBo3LCMiw/bhCEfT+agq/7NIde8wsjPpnLjL35A00YNALjv1THc9dJH3HTMD3j3kl48eMoPeXRI4VR6dafntfzVx4uG5wG3V7cipXQTsLDSafoc/yfJzaSJE/nn889xxll/XKr7vTHodSaMH0/P3XaoWDZ//nyuufJy7u1/F08888+6HlVaoY2fWjii7tCyMWOnzK5Y3qFlY8YXj7bHTS0s/6DS1eHzFyQ+Gj+DNdsUTl/vv/madGnfjEOufoUFxX9S/7f/UIZcsAd7brI6/3ijEOWLH32XSx97l46tGjNp+tdsu0EHAMZMnFXaJ6paKUmsI2LY4lYBq5fiMbV8PPKPgTRq1JBee++zVPc75NAj2G2PnlWWnXzCsfTstQ8HHHRIXY4orRTGTJrFuKmz2b5bB4aNKbzFqlF5GVt0bUvfR94BYPiYqcyZO5+uqzVn8OjJAERAlw7NeLH4WneThg1IiYpQQ+HrBJRFVHnMBQm+/Krwg8B+3+/EG6MnM2nG1yV+pqqNUh1Zrw70BCYvsjyAf5foMVViKSX+PnAAe/bah2bNmldZN3PmDMZ88gkAC9ICvvj8c959521atW5Np05r0q59e9q1b1/lPuXl5bTv0IF11vWKU62amjVqwNodCq87l0WwZtumfHfNlnw1cy5jp8zm9hc/4sTd1uPDcdMZPX4GJ+2xPjPnzOfh4tXe0+fM455XPuGUnhvw+ZTZfDppFj/bfm1aN23IwMGF92u/9N4Efr9fd84/6Hvc8a+PKIvg+N3WY8GCxCvvTwSgbfOG7N2jE69+MJFG5WUcvNV32HuzThx23av1843Rt5Qq1o8CLVJKQxddEREvlOgxVWKDB73GJx9/zPkXXfqtdSNHDOe4Y46quH3j9ddw4/XXsO+Pfsx5F/RdnmNKK4xNOrfm3pO2qbj9m17d+E2vbjzw+qf87r5h3PjcKJo0bMB5B32P1k0bMvSTKRx14+sV77EG+PPD7/D1vAVcdngPmjQqY8SnUzni+tcqTpWPGjeDX976Br/ec30ePGVbUoKRY6dy9M2D+OKrb06vH7DFWpy5X3cCGPLxFA6/7tUqH5qi+hUp34sHfM1aqgctGgddT328vseQVkmjrtg7qlvuW7ckScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyZ6wlScqcsZYkKXPGWpKkzBlrSZIyFyml+p5BK6GI6J1Suqm+55BWNf7dWzl5ZK1S6V3fA0irKP/urYSMtSRJmTPWkiRlzlirVHzNTKof/t1bCXmBmSRJmfPIWpKkzBlr1amI6BUR70bEBxFxZn3PI60qIuK2iBgXEcPrexbVPWOtOhMRDYDrgL2AjYDDI2Kj+p1KWmXcAfSq7yFUGsZadWkr4IOU0qiU0tfAfcD+9TyTtEpIKb0ITKrvOVQaxlp1aS1gTKXbnxaXSZL+C8ZakqTMGWvVpc+AzpVuf6e4TJL0XzDWqkuDgA0iYt2IaAQcBjxczzNJ0grPWKvOpJTmAScDTwFvA/enlEbU71TSqiEi7gVeATaMiE8j4pj6nkl1x08wkyQpcx5ZS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMt1ZOImB8RQyNieEQMiIhm/8W+7oiIg4tf31LTL1CJiJ0jYttleIyPIqJDbZcvss30pXysP0XEaUs7o7SyMtZS/ZmVUtospbQx8DVwfOWVEVG+LDtNKR2bUhpZwyY7A0sda0n1x1hLefgXsH7xqPdfEfEwMDIiGkTEpRExKCKGRcRxAFFwbfF3hz8DrLZwRxHxQkRsUfy6V0S8GRFvRcSzEbEOhR8KflM8qt8hIjpGxIPFxxgUEdsV79s+Ip6OiBERcQsQS3oSEfH3iHijeJ/ei6z7S3H5sxHRsbhsvYh4sniff0VE92r2+euIGFl8/vct4/dXWqEt00/ukupO8Qh6L+DJ4qLNgY1TSqOLwfsqpbRlRDQGXo6Ip4HvAxtS+L3hqwMjgdsW2W9H4GZgx+K+2qWUJkXEX4HpKaXLitvdA/wlpfRSRHSh8Al03wXOBV5KKfWJiH2A2nwi1i+Kj9EUGBQRD6aUJgLNgcEppd9ExDnFfZ8M3AQcn1J6PyK2Bq4Hdl1kn2cC66aU5kREm9p8T6WVjbGW6k/TiBha/PpfwK0UTk+/nlIaXVy+J7DpwtejgdbABsCOwL0ppfnA2Ih4rpr9bwO8uHBfKaXF/a7j3YGNIioOnFtFRIviYxxYvO9jETG5Fs/p1xFxQPHrzsVZJwILgL8Vl/cHHio+xrbAgEqP3biafQ4D7o6IvwN/r8UM0krHWEv1Z1ZKabPKC4rRmlF5EfCrlNJTi2y3dx3OUQZsk1KaXc0stRYRO1MI/w9TSjMj4gWgyWI2T8XHnbLo96Aa+1D4wWE/4OyI2KT4OfTSKsPXrKW8PQWcEBENASKiW0Q0B14EDi2+pt0J2KWa+74K7BgR6xbv2664fBrQstJ2TwO/WngjIjYrfvkicERx2V5A2yXM2hqYXAx1dwpH9guVAQvPDhxB4fT6VGB0RBxSfIyIiB6VdxgRZUDnlNLzwBnFx2ixhDmklY6xlvJ2C4XXo9+MiOHAjRTOiA0E3i+uu4vCb1uqIqU0HuhN4ZTzW3xzGvoR4ICFF5gBvwa2KF7ANZJvrko/j0LsR1A4Hf7JEmZ9EiiPiLeBvhR+WFhoBrBV8TnsCvQpLj8SOKY43whg/0X22QDoHxH/AYYAV6eUpixhDmml42/dkiQpcx5ZS5KUOWMtSVLmjLUkSZkz1pIkZc5YS5KUOWMtSVLmjLUkSZkz1pIkZe7/ATWGdwaLZm3lAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "# Plotando a matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            annot_kws={'fontsize': 14}, linewidths=0.5)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m812/812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 819us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAAsTAAALEwEAmpwYAABELElEQVR4nO3deZzN9ffA8deZsczYGVsRoyK7oQklWbJmq18LbdKXlm/lWylRSSqtSpvSJloUlQrZQkRCBkOWSLYG2bcZZj+/Pz6fme4wq5k7d2bueT66j+5nP597xz2f92c5b1FVjDHG+K8AXwdgjDHGtywRGGOMn7NEYIwxfs4SgTHG+DlLBMYY4+csERhjjJ+zRGBSicgoEfnc13EUViKyUUTa+zoOABF5UUQeyoP1lBSRP0SkSh6EZQooSwRFkIjsFJFOHsP9ROSoiLTzZVwFkYgMdH/oTorIfhGZLSJlz2VdqtpIVRfncYg55v5o9wfed4fbi0iUx/QSIvKtiCwTkXIiUkFEPhaRf9zPYauIDAdQ1TjgY2B4FtusJyJfi8ghETkuIutFZIiIBHpvT01esURQxInIHcA7QA9V/dnX8eSWiBTLw3W1A14AblbVskADYGperf+MbeVZ3NkwAJitqqfTiaMk8C1QAeiiqieA14EyOPtfHugNbPNY7AvgDnfZs4jIRcBK4G+giaqWB24EwoEcJ9V8/qwMlgiKNBG5B3gN6Kqqv7rjzheRGSJyRES2ichdGSwbKiIqIneKyN9ui+JeEbnMPdo7JiLjPOYPEJERIrJLRA6IyKciUt5jen932mERecqz1eKefnhDRPa6rzdSfnRSjmZFZJiI/ANMzGL+zSLS02O7xUTkoIi0SGc3LwOWq+paAFU9oqqfqOpJd9lJIvKuiMwRkWj3CLq6u72jbkuiuce2PPdplIh8IyKfi8gJYICILBaR59z1nBSRH0Wkssfyvd3TS8fceRu444eJyDdnfD9vishbGXz13YGzkr6IlAJmAsVwDgxiPD6HL1T1qKomq+ofqpq6PVWNAo4CrTPY3jPAr6o6RFX3uctsUdVbVPXYmS2SbHxWT4jIaRGp5DF/c7e1Udwd/o/7XR8VkXkiUjuD2Ew2WCIouv4LPAtcraoRHuOnAFHA+cANwAsi0jGT9bQC6gJ9gTeAJ4FOQCPgJvn3dNMA99UBuBDnCHMcgIg0BN4FbgXOwznqrOGxjSdxfmTCgGZAS2CEx/TqQCWgNnB3FvN/CdzssWxX4JCqrkln31YCXUXkGRFpk8ER703uuisDccByYI07/A0wNp1lUvRx56kATHbH3QLcCVQFSgCPgnNqxY39IaAKMBuYKSIlcL6za8Q9ZeWebrkJ50g9PU2ALWeMKwnMAWKBPme0FlYAz7tJv24G69yM81mnp5O7n7nh+VmNwfmcr/eYfgvwjaomiEgf4Ang/3A+q6U4n505V6pqryL2AnYCJ4DpQIDH+AuAJKCsx7gXgUnu+1HA5+77UECBGh7zHgb6egxPAx5y3y8E7vOYdgmQgHP0ORL40mNaKSAe6OQO/wVc4zG9K7DTfd/enTfIY3pm818MnARKucOTgZGZfFbdcY6SjwHROD/sge60ScCHHvMOBjZ7DDcBjp3xuafs0yhgyRnbWgyM8Bi+D5jrvn8K+MpjWgCwB2jvDv8C9Hffdwb+ymSfEoD6HsPtcRJAPHB9OvMH4/ywrnaX3QZ0P2OeDD9Hd5lumcTTHohK5280s89qEPCT+15wTjtd5Q7PAQae8VmdAmr7+t9eYX1Zi6Do+i9QD/hIRMQddz5wRN1TH65dpD06P9N+j/en0xku47HuXWestxhQzZ32d8oEVT2Fk1TIZNnzPYYPqmpsduZX1W04R6+93FMhvXGPnN3TOymvWu78c1S1F06Low9Oq2bQOex/ev5OZ9w/Hu9PkcHnp6rJ7vIp380X/NvSuYWMWwPgnMY589z8IaAf8ImIdPWcoKqnVfUFVb0UCAG+Ar72PDXjru9YBts7jNPSy40zP6tpwOUich5wFZCMc+QPTsvwTfcU2jHgCE6yyOzv2GTCEkHRtR+4GmiLc1oGYC9QSdLeFVML58gzt/bi/AP1XG+iG8c+oGbKBBEJxvnByWzZvR7DZ5bIzWr+lNNDfYBNbnJAVct4vHZ7rlCdc+MLgZ+AxpnvarblpLRvmn1yk/cF/PvdfA20F5GawHVkngjW4xwEpA1G9VvgLuAbEemQbsDOxeMXgNJAHY9JDYB1GWxvAWlP45wpBqcVCKSe2jrzdtQ0n5WqHgV+xDkleQswRd3Df5ykcY+qVvB4Bat7HczknCWCIkxV9+Ikg24i8rqq/g38CrwoIkEi0hQYCOTFswNfAg+LSB0RKYPzYzJVVRNxzv32EpEr3HPeo3CO4DyXHSEiVdyLpyOziCmr+acAXXBaRRn+YIpIH3Fura0ojpZAO5xz5vntK6CHiFztXhB9BOeaxK8AqnoQ59TSRGCHqm7OZF2zcfbjLKr6JfAAMF1E2gCIc/H+MnFuKw0CHsQ5+t/iTq+B02LK6HN5GrhCRMaISHV3mYvdi78VgK1AkIj0cPdtBM41i6x8gXMb7A2k/R7fAx4XkUbutsqLyI3ZWJ/JgN2mVcSp6m73YvASEYnFOVJ+D+cI9CjwtKouyINNfYxzemMJEATMwzmnjqpuFJHBOD/QpXEuOh/A+aEDGA2UwzmSBefod3Qm28p0flXdJyLLcX4Mb8pkPUeB/+Fc1C6J03IZo6qTM1nGK1R1i4jcBryNc4ojEuilqvEes30BfAo8lsXqPgUiRSRY07mFVFU/cRPyLBHpgnM0PpF/W3Hrce4qinYXuQX4RJ1nCtKL/S8RuRznO9gozu2fO911nlTVJBG5D/gICARewblhISsz3GV2q2pqa0RVv3MPNqa4dwsdB+bj/B2YcyD/traMyR/uP+JjQF1V3eHjcIokEXkBOKCqb+RyPSVxTgldpaoH8iI2U/BYIjD5QkR64dxZJDjPNrQCWqj9ARrjc3aNwOSXPjino/biPJfQz5KAMQWDtQiMMcbPWYvAGGP8XKG7a6hy5coaGhrq6zCMMaZQWb169SFVTbeceKFLBKGhoURERGQ9ozHGmFQisiujaXZqyBhj/JwlAmOM8XOWCIwxxs9ZIjDGGD9nicAYY/yc1xKBOJ1hHxCRDRlMFxF5S5zuEtdL+l0JGmOM8TJvtggmAd0ymd4dp9RAXZzuB8d7MRZjjDEZ8NpzBKq6RERCM5mlD/CpW29mhYhUEJHz1O382hR8yclKsipJqiQn4/FeSVaIT0xOHQZnerI6/1d1uklNVlD+XV7d4ei4RIoFBDhd6bnL4vyXOl+yOw3PdXpMB2f9R0/FU6pEYGrcKVVVUv+fOl7TDOPGkv68KcPpTD9jPf9uL4PxHtPOihHl+OkEAkUoWTyQnMpNCZlzXTQ3RWvOfZuFaD/PYYPFkmK5/O8PKX3lvTRsmFf9JnmsP8/XmH01SNs9XZQ77qxEICJ347QaqFWrVr4EVxgkJCVzOiGJ6NhETsUnEZuQRExcInuPnyY6NpHYhGT+PnqKUiWKEZeYxI5DMVQsVYLEZCUxKZmEJCU2IYl9x09TqXQJEpKUxORkEpOUPcdOU6pEIIKQpEqSu0xSshITn+TrXTemUBPJep4UrWUjLxX/kNpygJUbakERSwTZpqofAB8AhIeHF9kqearKidOJxMQnsu1ANAdOxrHrcAyJycqBE3HsPBzDjkMxJCQmczIuMUfrLhtUjOKBARw9FU+dkNIEBgjFAgMoHigUCwjgZGwiVcqWpHhgAMUChAurlObgyThCQ0pTzB0XGCAUC3D+gmPik6heLogAgYAAIUCEwAAIkJT3QrIqCUnJVChVwh3vTBcBcYcF9/+S9v+qkKRKmZLFnPlxlktZ3gnDYxykvsfjfcqyACWKBaR2i5byDzFl2pn/MFOni2S5TOqiHtPPnJbSbXRG60LOXp/nMoHuZ3wuznGxNLHkfJvnvtFz3+Y5bzJX8XpN7HH48SlY8wlUuhB6T6RV6JVe2ZQvE8EenD5ZU9Qkb/rOLfBiE5JYvOUAUUdPE/n3MbYdiCY+MZn9J2IzPNquWrYkZYOKUb96WUSgerlgqpcvSYXgEgSXCKRccHFKFgsguHgglUqXoGxQMSoEl6BUyUCKB9rNYcYUKn/MhllDIHo/tHkQ2j8OxYO9tjlfJoIZwAMiMgWnk5LjRfH6wPaD0Szffpg/9p1k+fbD7DocQ0LSv42aCqWKU61sEDUqBtP6ohAuqFiKiqWKEyBCveplqV2pFBVKFS+YRyzGmLwVfRDmPAYbv4WqjaDfF1DD+zdUei0RiMiXQHugsohE4XRwXRxAVd/D6WD7GmAbcAq401ux5LeNe4+zYvsRZq3fy5rdxwAoXSKQC6uUofkFFalTuTRXXBxCi1oVqVkx2H7kjfF3qvD71zBnGMRHQ4cRTkugWIl82bw37xq6OYvpCtzvre3nN1Vlwi87mLhsJ3uOOf2F16gQzNCul3B1g6rUq1qWgAD7wTfGnOF4FPwwBP6cBzUvg97joGr9fA2hUFwsLqiSk5XIqGMs3LyfryOiOHAyjgbnlePONqF0rF+VC6uU8XWIxpiCKjkZVk+E+U+DJkG3l6Dl3RCQ89uEc8sSQQ4lJSvTI/cQsesoCzfvZ/+JOABa1KrAQ53q0e+yC+zI3xiTucN/wYzBsGsZXNgeer0JFUN9Fo4lgmxQVdbsPsrXEVHMXLc39c6eNheHMKRzPdpfUpVq5YJ8HKUxpsBLSoTl42DxixBY0jkN1Py23N37mgcsEXhKSoI5c2DtWmjenKSu3Zi/5RBj529h6/5oAK6qV4UeTapzbfMalCyW/004Y0wh9c/vMP0B2BcJ9XvCNa9CufN8HRVgieBfSUnQtSusXInGxJBQMpi159XjvhueoVK5UjzTuxHdm1Snalk78jfG5EBiHCwZA7+8DsEV4cZJ0PBan7cCPFkiSDFnDrpyJRIdjQAlYk/RdO8WPqxykCsfHmBH/8aYnPv7N6cVcGgLNLsZur4ApSr5OqqzWCJwxf4WQYnomDSPtwfHx3J13D6wJGCMyYn4GFj4HKx8D8rXhFunQd1Ovo4qQ5YIgHV/H+OzXcV5pnhJSifE/juhdGkIC/NZXMaYQuivRTDzf3BsN1x2F3R6GkqW9XVUmfL7IjQ7DsVwx8TfWHpxOImXXQZlyoAIoSIsqFsXuncHYMqUKVSsWJGff/6ZCRMmUL9+fcqWLUu1atW45pprOHnyZI62q6oMGzaMkJAQQkJCGDZsWIYlg1WV559/nlq1alGuXDn69evHiRMnUqd/9dVXXHHFFZQqVYr27duf82dhjMmF00dh+v3w2bUQWALunAM9Xi3wSQD8PBHEJiTR/+OVxCUkM/6OlpRfsgi+/BKefRaqVIEXX4TAQD755BPuv/9+Zs2aBcATTzzBl19+ycmTJ9m8eTN9+/bN8bY/+OADvv/+e9atW8f69euZOXMm77//frrzfvrpp3z22WcsW7aMvXv3cvr0aQYPHpw6vVKlSjz00EMMHz783D4IY0zubJ4J77SCyC/hyofh3mVQ+wpfR5V9qlqoXpdeeqnmlTFz/9Daw37Q+Rv/OWta7dq1df78+free+9pSEiIrlq1yllmzBjt06dPrrd9+eWX6/vvv586/NFHH2mrVq3Snff666/XV155JXV42bJlWrJkSY2JiUkz34cffqjt2rXLdWzGmGw6uV91an/Vp8upjm+jumetryPKEBChGfyu+m2LYNfhGN77+S+6NqpGp4bV0p1n/PjxjBw5koULFxIeHg5Aq1atmDdvHk8//TTLli0jLi4uzTIvvfQSFSpUyPCVYuPGjTRr1ix1uFmzZmzcuDHDeNXjtJGqEhcXx59//nkuu26MyS1V5+h/3GWwZTZ0fAruWgTnh/k6snPit4ngsW/WAzCsW8bFnebPn0/r1q1p0qRJ6ri2bdvy7bffsmbNGnr06EFISAhDhgwhKcl52nj48OEcO3Ysw1eK6Ohoypcvnzpcvnx5oqOj071O0K1bNz766CN27tzJ8ePHefnllwE4depUrj4DY8w5OPY3TL4Bvr8XqlzinAa66lEILO7ryM6ZXyaCVTuPsHLHEbo0qpZpYbjx48ezdetWBg0alOYHunv37sycOZMjR44wffp0Jk2axEcffZSjGMqUKZPmgu+JEycoU6ZMuiWp//Of/3DzzTfTvn17GjVqRIcOHQCoWbNmjrZpjMmF5GT47UN4tzXsWg7dx8Cdc6FKPV9Hlmt+lwgSk5IZ5rYGRl/bJNN5q1WrxsKFC1m6dCn33XffWdMDAgK4+uqr6dixIxs2bADghRdeoEyZMhm+UjRq1Ih169alDq9bt45GjRqlG0dAQADPPPMMO3fuJCoqikaNGlGjRg1q1KiR4/03xpyDQ3/CpGtg9qNwQUu4bzm0uhsCisZPaNHYixxYuu0Q2w/F8FTPhlQqnXWnD+effz4LFy5k7ty5PPzww0yfPp0pU6Zw9OhRVJXffvuNn3/+mdatWwPOHUXR0dEZvlL079+fsWPHsmfPHvbu3ctrr73GgAED0o3hyJEj/PXXX6gqmzZtYsiQIYwcOZIA948wKSmJ2NhYEhMTSU5OJjY2loSEhNx/WMb4u6QEWDoWxreBA5vh2vFw27dQsbavI8tbGV1FLqiv3N419PysTVpn+A96MjYh0/lS7hpKsX37dq1Zs6a2b99eO3bsqCEhIVqmTBmtW7euvvzyyzmOIzk5WYcOHaoVK1bUihUr6tChQzU5OTl1eunSpXXJkiWqqrplyxatV6+eBgcHa61atfS1115Ls66JEycqkOZ1xx135DgmY4yHvZGq46907giacpvqibPvLixMyOSuIdEMHmIqqMLDwzUiIuKcl7/mzaUUCxRmPHBlHkZljCkyEmJhySvwyxtQKsR5KKxhH19HlWsislpVw9Ob5lclJhKSktm07wQ3t6zl61CMMQXR7hVOkbjDf0LYbdDluQJZJC6v+VUi2HPU6Uu4WrmSPo7EGFOgxJ2Ehc86dwWVv8C5DnDx1b6OKt/4VSJYv+c4AC3rFP0Mb4zJpm0LYOZDTifyre5xHg4r6V/9jftVIjgc7TwFXDuktI8jMcb43KkjMO9JWPcFVK4H/5kLtVr7Oiqf8KtEcPCkkwiqlLFTQ8b4tU3TYdajcOowtH0UrhoKxf2390G/SgT/nIilWrmSlCjmd49PGGMATv7jPBS2eSac1wxumwbnNfV1VD7nV4ng4Mk4KltrwBj/owqRX8C8x53bQzuNgssHQ6Bf/QRmyK8+hcPR8VS1O4aM8S9Hd8HMB2H7Iqh1BfR+Gypf7OuoChS/SgS7DsfQuEY5X4dhjMkPyUnO7aALnwURuOZVCB9YZOoD5SW/SgQx8UkkJheuJ6mNMefg4BaYMRj+XgkXd4Keb0CFC3wdVYHlN4kg2U0A1cv5750BxhR5SQmw7A34+RUoURquex+a9nVaBCZDfpMI4pOSAShd0m922Rj/snctTB8M+3+HRtdB91egTFVfR1Uo+M2vYoKbCEraraPGFC0Jp2HxS/Dr21C6CvSdDA16+jqqQsWrv4oi0k1EtojINhEZns70WiKySETWish6EbnGW7Gcjne6kkyyawTGFB07lzl9BSx7A8JugftXWhI4B15rEYhIIPAO0BmIAlaJyAxV3eQx2wjgK1UdLyINgdlAqDfiiUt0WgT2MJkxRUDsCVj4DKz6CCrUhv7T4cL2vo6q0PLmqaGWwDZV3Q4gIlOAPoBnIlAg5X7O8sBebwWTcrdQxVJZ90pmjCnA/pzvFIk7sQda3wcdRzgXhs0582YiqAH87TEcBbQ6Y55RwI8iMhgoDXRKb0UicjdwN0CtWufWl0DKNYLigdYiMKZQOnUE5j4O66dAlfowcD5ccJmvoyoSfP2reDMwSVVrAtcAn4nIWTGp6geqGq6q4VWqVDmnDcUmONcIigfabWTGFCqqsOFbGHcZbPgG2g2De5ZYEshD3mwR7AE8n+Co6Y7zNBDoBqCqy0UkCKgMHMjrYGLinESQcq3AGFMInNgHsx6BLbPg/ObQezpUb+zrqIocbyaCVUBdEamDkwD6AbecMc9u4Gpgkog0AIKAg94IJsBtCFQqbdcIjCnwVGHtZzBvBCTFQefnnOsBViTOK7z2qapqoog8AMwDAoGPVXWjiDwLRKjqDOAR4EMReRjnwvEAVfXK/Z1J7moD7AlDYwq2Iztg5v9gxxKofSX0fgtCLvJ1VEWaV9Orqs7GuSXUc9xIj/ebgDbejOHfbTn/DwywRGBMgZScBCvfh5+eAwmEnq9DiwFWJC4f+E07K+VBMssDxhRABzbD9AdgTwTU7eokgfI1fB2V3/CfRJByasgygTEFR2I8/PI6LBkDJcvC/30ETW6wInH5zG8SQcqlh0D7AzOmYNiz2ikSd2AjNL4Bur8MpSv7Oiq/5DeJwH2ezC4WG+Nr8adg8Quw/B0oUx1ungKXdPd1VH7NjxJByqkhHwdijD/bsdS5I+jIdrh0AHR+FoLK+zoqv+c3iSClxITdNWSMD8Qeh/lPw+qJULEO3DET6lzl66iMy28SwfHTCQAk24PFxuSvLXPhh4ch+h+4/AHo8CSUKOXrqIwHv0kEZdyeyYJLBPo4EmP8RMwhmDPMqQ9UtSH0/RxqXurrqEw6/CYRGGPyiSpsmAZzHnP6DWj/BFz5MBSz8i4FlSUCY0zeOb4HZg2BrXOhxqXQexxUa+jrqEwWLBEYY3IvORnWfALzR0JSAnR9AVrdCwF2KrYw8JtEoFhfxcZ4xeG/YOaDsHMphLZ1isRVutDXUZkc8JtEkMJuHjUmjyQlwsrx8NPzEFgcer0FLfpbeYhCyO8SgTEmD+zf6BSJ27sGLrkGerwG5c73dVTmHFkiMMZkX2IcLH3NeQVVgBs+hkb/Z62AQs4SgTEme6IinFbAwc3QtC90fRFKh/g6KpMHLBEYYzIXH+NcB1jxrnP655avoF5XX0dl8pDfJALvdIBpTBG3/WenSNzRnRA+EDqNgqByvo7K5DG/SQQp7FSmMdlw+hjMfwrWfAqVLoIBsyD0Sl9HZbzE7xKBMSYLf8yCH4ZAzAFo8yC0fxyKB/s6KuNFlgiMMY7og059oI3fQrXGcPOXUKOFr6My+cASgTH+ThXWfwVzhzkXhjuMgCsfch4SM37BbxKBXSw2Jh3Ho5y+Av78EWpe5hSJq1rf11GZfOY3iSCFWJEJY5wicas/hvmjQJOg20vQ8m4rEuen/C4RGOP3Dm2DGYNh969wYXvo9SZUDPV1VMaHLBEY4y+SEmH5OFj8IhQrCX3egbBb7Z5qY4nAGL/wz+8w/X7Ytw7q93SKxJWt7uuoTAFhicCYoiwxDpaMgV9eh+CKcOMn0LCPtQJMGn6TCOymIeN3dq90rgUc2gLNbnZ6DStVyddRmQLIbxJBCjsQMkVeXDT89BysfB/K14Rbp0HdTr6OyhRgAd5cuYh0E5EtIrJNRIZnMM9NIrJJRDaKyBfejMeYIu+vn2D85bDyPWh5F9y33JKAyZLXWgQiEgi8A3QGooBVIjJDVTd5zFMXeBxoo6pHRaSqt+Ixpkg7fRTmjYDIzyGkLtw5F2pf7uuoTCHhzVNDLYFtqrodQESmAH2ATR7z3AW8o6pHAVT1gBfjMaZo2jwTZj0CMYfgyiHQbhgUD/J1VKYQ8WYiqAH87TEcBbQ6Y556ACKyDAgERqnq3DNXJCJ3A3cD1KpV65yCUasxYYqak/thzlDYNB2qN3E6jDk/zNdRmULI1xeLiwF1gfZATWCJiDRR1WOeM6nqB8AHAOHh4faLbvybKqz7EuY+Dgmn4eqRcMX/rEicOWfeTAR7gAs8hmu64zxFAStVNQHYISJbcRLDKi/GZUzhdWw3zHwI/loIF7SG3m9DlXq+jsoUct68a2gVUFdE6ohICaAfMOOMeb7HaQ0gIpVxThVt92JMxhROycmw8gN4pzXsXgHdx8CdcywJmDzhtRaBqiaKyAPAPJzz/x+r6kYReRaIUNUZ7rQuIrIJSAKGquphb8VkTKF06E+Y/gD8vQIuuhp6vQEVzu1amTHp8eo1AlWdDcw+Y9xIj/cKDHFfXmUXFkyhk5QAv74Fi192uoq8drzzhLA9FWnymK8vFuc7+zdkCoV965wicf/87tQG6j4GylbzdVSmiPK7RGBMgZYQCz+/BMveglIhcNNn0LC3r6MyRZwlAmMKil3LYcYDcHgbhN0GXUc7FUON8TJLBMb4WtxJWPAMrPrQuQh8+3dwUUdfR2X8iCUCY3xp2wLnuYDjUdDqXuj4FJQs4+uojJ/JdiIQkWCglqpu8WI83mO3DZmC5NQRmPeE84Rw5Xrwn3lQ68wKLMbkj2w9UCYivYBIYK47HCYiZz4cViiI3TZkfEkVNn4P77SE37+Gto/CPUstCRifym6LYBRONdHFAKoaKSJ1vBSTMUXTyX+cKqF//ADnNYPbvoXzmvo6KmOynQgSVPX4GUfTdrLFmOxQhcjJzqmgxDjo9Axc/gAE2iU6UzBk9y9xo4jcAgS6ncn8D/jVe2EZU0Qc3QkzH4Tti6HWFU6RuMoX+zoqY9LIbtG5wUAjIA74AjgOPOitoLxBrQFj8lNyEqx4D969HKIioMdrMGCWJQFTIGW3RdBDVZ8EnkwZISI3Al97JSovskvFxusObnGKxEX9Bhd3hp6vQ4ULsl7OGB/Jbovg8WyOM8Z/JSXAz2PgvSvh8J9w3Qdw69eWBEyBl2mLQES6A9cANUTkLY9J5YBEbwZmTKGyd63TCti/ARr9H3R/BcpU8XVUxmRLVqeG9gIRQG9gtcf4k8DD3grKmEIj4TQsfhF+fRtKV4V+X0D9Hr6OypgcyTQRqOo6YJ2IfOF2J2mMSbFzGcwYDEf+ghb9ofNzEFzB11EZk2PZvVgcKiIvAg2BoJSRqnqhV6LyArWbhkxeiT0BC0ZBxASoUBv6T4cL2/s6KmPOWXYTwUTgaeB1oANwJ97t79hrrMKEyZWtP8IPD8GJvdD6fuj4JJQo7euojMmV7CaCYFVdKCKiqruAUSKyGhiZ1YLGFAkxh2HucPj9K6hSHwbOhwsu83VUxuSJ7CaCOBEJAP50O6TfA1itXFP0qcLGb2H2YxB7DNoNg7aPQLGSvo7MmDyT3UTwIFAKp7TEc0BHoL+3gjKmQDixD2YNgS2z4fzm0GcGVGvk66iMyXPZSgSqusp9Gw3cKSKBQD9gpbcCy2t2rdhkmyqs+RR+fAqS4qDLaGj1XysSZ4qsrB4oKwfcD9QAZgDz3eFHgPXAZG8HmNfEikyYzBzZATP/BzuWQO0rofdbEHKRr6MyxquyOsT5DDgKLAcGAU/glOu5TlUjvRuaMfkoOQlWvgcLn4OAYtDzDWhxBwQUypvjjMmRrBLBharaBEBEPgL24XRXGev1yIzJL/s3wYwHYM9qqNvVKRJXvoavozIm32SVCFKfJlbVJBGJsiRgiozEePhlLCx5FYLKwfUToPH19rCJ8TtZJYJmInLCfS9AsDssgKpqOa9GZ4y37FntFIk7sAma3AjdXoLSlX0dlTE+kVWtocD8CsTbrMSEASD+FCx6Hla8C2Wqw81T4JLuvo7KGJ/yu/vhrNXvx3YsgRn/g6M74NI7ofMzEFTe11EZ43N+lwiMH4o9DvNHwupJULEO3DET6lzl66iMKTAsEZiibcsc+OFhiN4PVwyG9k9AiVK+jsqYAsWrN0mLSDcR2SIi20RkeCbzXS8iKiLh3ozH+JGYQ/DNQPiyHwRXgkELnCeELQkYcxavtQjcMhTvAJ2BKGCViMxQ1U1nzFcWp5aRV8tVqBWZ8A+q8Ps3MOcxiDvptACufBiKlfB1ZMYUWN5sEbQEtqnqdlWNB6YAfdKZ7zngZSBfnk+wa8VF2PE9Tgvg20FQ6UK4dym0H2ZJwJgseDMR1AD+9hiOcselEpEWwAWqOiuzFYnI3SISISIRBw8ezPtITeGWnAwRH8M7rWD7z9D1BRj4I1Rt4OvIjCkUfHax2O3fYCwwIKt5VfUD4AOA8PBwO8dj/nX4L+eW0F2/OHcC9XoLKtXxdVTGFCreTAR7gAs8hmu641KUBRoDi8W5ub86MENEeqtqhBfjMkVBUqLzUNii5yGwJPR+G5rfbg+KGHMOvJkIVgF1RaQOTgLoB9ySMlFVjwOpz/SLyGLgUW8lAXuyuAj5Z4NTJG7vWrikB/R4Dcqd5+uojCm0vJYIVDXR7dZyHhAIfKyqG0XkWSBCVWd4a9uZsgPGwisxDpa+5ryCKsANE6HRddYKMCaXvHqNQFVnA7PPGJduh/eq2t6bsZhC7u9VTivg4B/QtK9TJK5UJV9HZUyRYE8Wm4ItPgZ+Gg0rxkO58+GWr6FeF19HZUyRYonAFFzbFzt3BB3bBeEDodMop98AY0yeskRgCp7Tx+DHEbD2M6h0EQyYDaFtfB2VMUWW3yQCu2mokPhjFvwwBGIOQpuHoP1wKB7s66iMKdL8JhGkELttqGCKPuDUB9r4HVRrArdMgfOb+zoqY/yC3yUCU8CowvqpMHe4c2G44winJRBY3NeRGeM3LBEY3zn2t9NXwLb5ULMl9BkHVS7xdVTG+B1LBCb/JSdDxARYMAo0Gbq9DC3vgoAi00W2MYWK/yQCqzFRMBzaBjMGw+5f4cIO0OsNqBjq66iM8Wv+kwhcVo3AR5ISYfnbsOhFKB4Efd6FsFvsCzGmAPC7RGB84J/fYfr9sG8d1O/pFIkrW93XURljXJYIjPckxMKSMbDsDaff4Js+hYbpdVJnjPElSwTGO3avdIrEHdoKzW6Brs9bkThjCihLBCZvxUXDwmfhtw+gfE24bRpc3MnXURljMuE3icDuGcoH2xbCzIfg+N/O7aBXj4SSZX0dlTEmC36TCFLYPSpecPoozHsSIidDSF24cw7UvtzXURljssnvEoHJY5tmwOxHIeYQXDkE2g1zbg81xhQalgjMuTm530kAm2dA9SZw69dwXjNfR2WMOQeWCEzOqELkFzDvCUg47VwHuOJ/ViTOmELMbxKBVZjIA0d3wQ8PwV8/wQWtoffbUKWer6MyxuSS3ySCFGIlDXIuORlWfQgLnnFKQlzzqtN1ZECAryMzxuQBv0sEJocObnWKxP29Ai662ikSV6GWr6MyxuQhSwQmfUkJsOxN+PllKF4Krn0PmvWzInHGFEGWCMzZ9kY65SH++d2pDXTNq1Cmqq+jMsZ4id8kArWrxVlLOO20AJa9BaUrw02fQcPevo7KGONlfpMIUtiJjQzsWu60Ag5vg+a3QZfREFzR11EZY/KB3yUCc4a4k87dQKs+dC4C3/49XNTB11EZY/KRJQJ/9ud8p0jciT3Q6r/QcQSULOPrqIwx+cwSgT86dQTmPg7rp0DlS2Dgj3BBS19HZYzxEUsE/kQVNn0Ps4c6FUOvGuq8ipX0dWTGGB/y6qOhItJNRLaIyDYRGZ7O9CEisklE1ovIQhGp7a1Y/P6eoZP/wNTb4OsBUK4G3L3YORVkScAYv+e1FoGIBALvAJ2BKGCViMxQ1U0es60FwlX1lIj8F3gF6OutmJy4vLn2AkgV1n7u9BeQFAedn4XW90OgNQaNMQ5v/hq0BLap6nYAEZkC9AFSE4GqLvKYfwVwmxfj8T9Hd8LMB2H7YqjdBnq9BZUv9nVUxpgCxpuJoAbwt8dwFNAqk/kHAnPSmyAidwN3A9SqZXVuspSc5PQZvPBZkEDoMRYuvdOKxBlj0lUgzg+IyG1AONAuvemq+gHwAUB4eLjfn+7P1IE/nAfDolbBxZ2dInHla/o6KmNMAebNRLAHuMBjuKY7Lg0R6QQ8CbRT1ThvBVPkK0wkxsOyN2DJGChRBv7vQ2hyox9eFDHG5JQ3E8EqoK6I1MFJAP2AWzxnEJHmwPtAN1U94MVY/t1mUSwysWeNUyp6/wZofD10exnKVPF1VMaYQsJriUBVE0XkAWAeEAh8rKobReRZIEJVZwBjgDLA126HMbtV1aqcZVfCaVj0AiwfB2WqQb8vof41vo7KGFPIePUagarOBmafMW6kx/tO3tx+kbbzF6cVcGQ7tLjDuS00uIKvozLGFEIF4mKxyYHYE7DgaYj4GCqGQv8ZcGG619iNMSZbLBEUJlvnwQ8Pw8l9cPkD0OEJKFHa11EZYwo5v0kEhfqmoZjDMHc4/P4VVKkPN30KNcN9HZUxpojwm0SQqjDdNKQKG6bBnMecU0LthkPbIVYfyBiTp/wvERQWJ/bCrEdgy2w4vwX0GQfVGvk6KmNMEWSJoKBRhTWfwI9PQVKC02Vk6/sgINDXkRljiihLBAXJke0w43+wcymEtoVeb0LIRb6OyuRCQkICUVFRxMbG+joU4yeCgoKoWbMmxYsXz/YyfpMItCDXmEhOghXj4afREFgcer7hPBtgReIKvaioKMqWLUtoaChi5T6Ml6kqhw8fJioqijp16mR7Ob9JBCkK3L/F/ZucInF7VkO9bk6l0PI1fB2VySOxsbGWBEy+ERFCQkI4ePBgjpbzu0RQYCTGwy9jYcmrEFQOrp/g1AmyH4wix5KAyU/n8vdmicAXolY7rYADm5wKod1ehtIhvo7KGOOn7CR0foo/5XQZOaETnD4GN0+F6z+yJGC85vDhw4SFhREWFkb16tWpUaNG6nB8fHyebKN9+/ZccsklNGvWjDZt2rBly5Ycr+Oaa67h2LFjHDt2jHfffTd1/N69e7nhhhtyHePOnTsJDg4mLCyMhg0b0r9/fxISEjJdZvHixfz666853tbatWsZOHAgEydOTP2sS5QoQZMmTQgLC2P48OFMmjSJKlWqpMbz4YcfAqQZX79+fV5//fXU9Y4bN46PP/44x/Fki6oWqtell16q5+LDJX9p7WE/6PHT8ee0fK5t/1n1jaaqT5dTnfGg6uljvonD5KtNmzb5OoRUTz/9tI4ZMybNuISEhFyvt127drpq1SpVVX3//fe1V69e57yuHTt2aKNGjXIdU2brTUxM1A4dOujnn3+e6TLpfV7ZccMNN2hkZGSacbVr19aDBw+mDk+cOFHvv/9+VVXdv3+/Vq5cWf/555804w8dOqQhISG6e/duVVWNiYnRsLCwbMWQ3t8dTtXndH9X7dSQt8Ued54JWPMJVKwDd/wAddr6OirjA8/M3MimvSfydJ0Nzy/H071y9qDhgAEDCAoKYu3atbRp04Zy5cpRpkwZHn30UQAaN27MDz/8QGhoKJ9//jlvvfUW8fHxtGrVinfffZfAwIyfabnqqqt44403UFUee+wx5syZg4gwYsQI+vbty759++jbty8nTpwgMTGR8ePH07ZtW0JDQ4mIiGD48OH89ddfhIWF0blzZ+6//3569uzJhg0baN26NRMmTKBRI2d/27dvz6uvvkqDBg0YPHgwGzZsICEhgVGjRtGnT58MYwwMDKRly5bs2eP0kzVz5kxGjx5NfHw8ISEhTJ48mdOnT/Pee+8RGBjI559/zttvv039+vW599572b17NwBvvPEGbdq0SbPukydPsn79epo1a5bt76Nq1apcdNFF7Nq1K834kJAQLr74Yvbt28cFF1xAqVKlCA0N5bfffqNly5bZXn92+N2poXy9bLdlDrzTCtZ+BlcMhv/+aknAFAhRUVH8+uuvjB07NsN5Nm/ezNSpU1m2bBmRkZEEBgYyefLkTNc7c+ZMmjRpwrfffktkZCTr1q1jwYIFDB06lH379vHFF1/QtWvX1GlhYWFpln/ppZe46KKLiIyMZMyYMWmm9e3bl6+++gqAffv2sW/fPsLDw3n++efp2LEjv/32G4sWLWLo0KHExMRkGGNsbCwrV66kW7duAFx55ZWsWLGCtWvX0q9fP1555RVCQ0O59957efjhh4mMjKRt27Y8+OCDPPzww6xatYpp06YxaNCgs9YdERFB48aNM/2MzrR9+3a2b9/OxRdfnGb87t27iY2NpWnTpqnjwsPDWbp0aY7Wnx3WIvCGmENOfaAN06BqI+g3GWpc6uuojI/l9Mjdm2688cZMj+wBFi5cyOrVq7nssssAOH36NFWrVk133ltvvZXg4GBCQ0N5++23GTt2LDfffDOBgYFUq1aNdu3asWrVKi677DL+85//kJCQwLXXXntWIsjMTTfdRJcuXXjmmWf46quvUq8d/Pjjj8yYMYNXX30VcH7od+/eTYMGDdIsn9LS2LFjBz169Ej9gY2KikptrcTHx2d4//2CBQvYtGlT6vCJEyeIjo6mTJkyqeP27dtHlSrZ6x1w6tSp/PLLL5QsWZL333+fSpUqpY5fsmQJf/zxB+PGjSMoKCh1mapVq/LHH39ka/05YYkgL6nC71/DnGEQdxI6PAltHoJiJXwdmTFplC79b/nyYsWKkZycnDqc8hS0qnLHHXfw4osvZrm+yZMnEx6edUXcq666iiVLljBr1iwGDBjAkCFD6N+/f7ZirlGjBiEhIaxfv56pU6fy3nvvpcY5bdo0LrnkkkyXT2lpHDp0iDZt2jBjxgx69+7N4MGDGTJkCL1792bx4sWMGjUq3eWTk5NZsWJFmh/mMwUHB2f7KfK+ffsybty4DMdHRETQpUsXevfuTfXq1QHnuwkODs7W+nPC704Nec3xKPiiL3x7F1S6EO5dCu0esyRgCrzQ0FDWrFkDwJo1a9ixYwcAV199Nd988w0HDjjdiR85cuSs89gZadu2LVOnTiUpKYmDBw+yZMkSWrZsya5du6hWrRp33XUXgwYNSt1uirJly3Ly5MkM19u3b19eeeUVjh8/nnpE37VrV95+++3U6gFr167NNLbKlSvz0ksvpSa448ePU6OG8xDnJ598kmEsXbp04e23304djoyMPGvdDRo0YNu2bZluP7vCw8O5/fbbefPNN1PHbd26NcennrLDbxKB1ypMJCfDqgnwTmunRlDXF2Hgj1C1QdbLGlMAXH/99Rw5coRGjRoxbtw46tWrB0DDhg0ZPXo0Xbp0oWnTpnTu3Jl9+/Zla53XXXcdTZs2pVmzZnTs2JFXXnmF6tWrs3jxYpo1a0bz5s2ZOnUqDz74YJrlQkJCaNOmDY0bN2bo0KFnrfeGG25gypQp3HTTTanjnnrqKRISEmjatCmNGjXiqaeeyjK+a6+9llOnTrF06VJGjRrFjTfeyKWXXkrlypVT5+nVqxffffcdYWFhLF26lLfeeouIiAiaNm1Kw4YNU1sknurXr8/x48czTWY5MWzYMCZOnJi6vmXLltG5c+c8WbcnUa/9QnpHeHi4RkRE5Hi5D5ds5/nZm9nwTFfKlMyjM2KH/3KKxO36Beq0c4rEVcp+fQ9T9G3evPmsc9WmaHv99dcpW7ZsuheTc2Pt2rWMHTuWzz77LMt50/u7E5HVqpru+Tu/aRHkqaREWPYmjL8C/vkder8N/adbEjDG8N///peSJfO+86hDhw7x3HPP5fl6wS4W59w/G5zyEHvXwiU9oMdrUO48X0dljCkggoKCuP322/N8vd44JZTCEkF2JcY5BeJ+GQvBFeHGSdDwWisSZ4wp9PwmEWhuuq//+zeY/gAc2gJN+0G3F6FUpbwLzhhjfMhvEkGKHB2/x8fAwudg5XtQrgbc+g3U9V7zzBhjfMHvEkG2/bUIZv4Pju2GywbB1U87/QYYY0wRY3cNnen0MZh+P3x2LQQUhwGznQvClgRMIRUYGJhaDjksLIydO3dmOK9nuYRzNWDAAOrUqUNYWBgtWrRg+fLlOV7HoEGDUss5vPDCC2mmXXHFFbmOEf79XBo3bkyvXr04duxYpvNHRkYye/bsHG9n37599OzZE3BKW5cvXz71u+jUqRMAo0aNSi0R3rhxY2bMmHHW+IYNG/Lll1+mrvfRRx/lp59+ynE86cqoLGlBfZ1rGer3f96mtYf9oNGxmZTd3TRTdUw91VEVVec/rRp/6py2ZUyKglCGunTp0l6ZNyN33HGHfv3116qqOm/ePG3SpEmu1pcXMWW13v79++vo0aMznd+zRHROPProo/r999+rquqiRYu0R48eZ83jWfJ606ZNGhISoklJSWnGb926VcuWLavx8U4p/Z07d2rnzp3T3aaVoT4X0Qdg9lDY9D1UawK3TIHzm/s6KlPUzBnuPHeSl6o3ge4v5WiR6Oho+vTpw9GjR0lISGD06NFnlW3OqFz0jz/+yNNPP01cXBwXXXQREydOzLQVcdVVV6WWXBg7dmxqxyqDBg3ioYceIiYmhptuuomoqCiSkpJ46qmn6Nu3b2qJ6W+++YbTp08TFhZGo0aNmDx5MmXKlCE6Opp+/fpx++2306NHD8BpifTs2ZPrrruO4cOHs3jxYuLi4rj//vu55557Mv1MLr/8ctavXw/Ab7/9xoMPPpha12fixInUqVOHkSNHcvr0aX755Rcef/xxevbsma3y19OmTWP06NHZ/n4aNGhAsWLFOHToUJrxdevWpVSpUhw9epSqVatSu3ZtDh8+zD///JNai+hc+U0iSPcBalVYNwXmDoeEU9DxKWjzIAQWz/f4jPGWlB9SgDp16vD111/z3XffUa5cOQ4dOkTr1q3p3bt3mr5uU8pFP/nkkyQlJXHq1CkOHTrE6NGjWbBgAaVLl+bll19m7NixjBw5MsNtp5SlXr16NRMnTmTlypWoKq1ataJdu3Zs376d888/n1mzZgFO3R9PL730EuPGjUu3rk9KWeoePXoQHx/PwoULGT9+PBMmTKB8+fKsWrWKuLg42rRpQ5cuXTKsKpqUlMTChQsZOHAg4JSJWLp0KcWKFWPBggU88cQTTJs2jWeffZaIiIjUQnFPPPEEHTt25OOPP+bYsWO0bNmSTp06pSnot2PHDipWrJjmAbOlS5emfh833ngjTz75ZJp4Vq5cSUBAwFlVTNesWUPdunXTVIBt0aIFy5Yt4/rrr8/wO8gOv0kEKVL/1o/9DT88BNsWQM2W0GccVMm8eqExuZLDI/e8EhwcnOaHNCEhgSeeeIIlS5YQEBDAnj172L9/f5qjyvTKRf/8889s2rQptTOW+Ph4Lr/88nS3OXToUEaPHk2VKlWYMGECCxcu5Lrrrkv9kfy///s/li5dSrdu3XjkkUcYNmwYPXv2pG3b7PfX0b17dx588EHi4uKYO3cuV111FcHBwfz444+sX7+eb775BnCSy59//nlWIkhJkHv27KFBgwapD2wdP36cO+64gz///BMRybBLy+yUv06vLHXbtm354Ycfzlrf66+/zueff07ZsmWZOnVqamJ+/fXXmThxIlu3bmXmzJlplqlatSp79+7N9meWEa9eLBaRbiKyRUS2icjwdKaXFJGp7vSVIhLqzXgA0GT47UN4tzXsWg7dX4H/zLUkYPzG5MmTOXjwIKtXryYyMpJq1aqdVTo5pVx0jRo1GDBgAJ9++imqSufOnYmMjCQyMpJNmzYxYcKEdLcxZswYIiMjmT9/fqbVMuvVq8eaNWto0qQJI0aM4Nlnn832fgQFBdG+fXvmzZvH1KlT6du3L+Bc93z77bdT49yxYwddunQ5a/mUBLlr1y5UlXfeeQdwith16NCBDRs2MHPmzAzLSqtb/jplO+n1gZCTstQpneAsXbo0TUJ8+OGH2bhxI9OmTWPgwIFp1pdXZam9lghEJBB4B+gONARuFpGGZ8w2EDiqqhcDrwMveysegAtlLyU/6wWzH4Wal8F9y6HVPRCQeQcdxhQlx48fp2rVqhQvXpxFixalW1o6vXLRrVu3ZtmyZann/GNiYti6dWu2ttm2bVu+//57Tp06RUxMDN999x1t27Zl7969lCpVittuu42hQ4eeVZYaoHjx4hkelfft25eJEyemti7AKUs9fvz41GW2bt2aaY9lpUqV4q233uK1114jMTExTVnqSZMmpc53Zlnq7JS/rlevXqZ3aeVE7969CQ8PT1MqO6/KUnuzRdAS2Kaq21U1HpgCnHklpQ+QslffAFeLeKdmwyX7vmdOiccJOLQZ+rwLt38HFWt7Y1PGFGi33norERERNGnShE8//ZT69eufNU965aKrVKnCpEmTuPnmm2natCmXX355tnvLatGiBQMGDKBly5a0atWKQYMG0bx5c37//XdatmxJWFgYzzzzDCNGjDhr2bvvvpumTZty6623njWtS5cu/Pzzz3Tq1IkSJZy+PwYNGkTDhg1p0aIFjRs35p577iExMTHT+Jo3b07Tpk358ssveeyxx3j88cdp3rx5muU6dOjApk2bCAsLY+rUqdkqf126dGkuuuiiPOujYOTIkYwdO5bk5GQSEhLYtm1btjoEyorXylCLyA1AN1Ud5A7fDrRS1Qc85tngzhPlDv/lznPojHXdDdwNUKtWrUuz2zmGp1U/z6LYqvdoMPB9giqef667ZUyOWBlq891337F69eoc3TmU3fWuWbMm3YqkOS1DXSguFqvqB8AH4PRHcC7ruKxdD2jXI0/jMsaYrFx33XUcPnw4z9ebmJjII488kifr8mYi2ANc4DFc0x2X3jxRIlIMKA/k/SdmjDE+lNed1IBz62le8eY1glVAXRGpIyIlgH7AjDPmmQHc4b6/AfhJvXWuyhgfsT9pk5/O5e/Na4lAVROBB4B5wGbgK1XdKCLPikhvd7YJQIiIbAOGAGfdYmpMYRYUFMThw4ctGZh8oaocPnyYoKCgHC3nN30WG+MLCQkJREVFZftecmNyKygoiJo1a1K8eNoKCYX+YrExhVXx4sUzLG1gTEFhZaiNMcbPWSIwxhg/Z4nAGGP8XKG7WCwiB4GcP1rsqAwcynKuosX22T/YPvuH3OxzbVWtkt6EQpcIckNEIjK6al5U2T77B9tn/+CtfbZTQ8YY4+csERhjjJ/zt0Twga8D8AHbZ/9g++wfvLLPfnWNwBhjzNn8rUVgjDHmDJYIjDHGzxXJRCAi3URki4hsE5GzKpqKSEkRmepOXykioT4IM09lY5+HiMgmEVkvIgtFpND305nVPnvMd72IqIgU+lsNs7PPInKT+11vFJEv8jvGvJaNv+1aIrJIRNa6f9/X+CLOvCIiH4vIAbcHx/Smi4i85X4e60WkRa43qqpF6gUEAn8BFwIlgHVAwzPmuQ94z33fD5jq67jzYZ87AKXc9//1h3125ysLLAFWAOG+jjsfvue6wFqgojtc1ddx58M+fwD8133fENjp67hzuc9XAS2ADRlMvwaYAwjQGliZ220WxRZBS2Cbqm5X1XhgCtDnjHn6AJ+4778BrhYRyccY81qW+6yqi1T1lDu4AqfHuMIsO98zwHPAy0BRqAOdnX2+C3hHVY8CqOqBfI4xr2VnnxUo574vD+zNx/jynKouAY5kMksf4FN1rAAqiMh5udlmUUwENYC/PYaj3HHpzqNOBzrHgZB8ic47srPPngbiHFEUZlnus9tkvkBVZ+VnYF6Une+5HlBPRJaJyAoR6ZZv0XlHdvZ5FHCbiEQBs4HB+ROaz+T033uWrD8CPyMitwHhQDtfx+JNIhIAjAUG+DiU/FYM5/RQe5xW3xIRaaKqx3wZlJfdDExS1ddE5HLgMxFprKrJvg6ssCiKLYI9wAUewzXdcenOIyLFcJqTh/MlOu/Izj4jIp2AJ4HeqhqXT7F5S1b7XBZoDCwWkZ0451JnFPILxtn5nqOAGaqaoKo7gK04iaGwys4+DwS+AlDV5UAQTnG2oipb/95zoigmglVAXRGpIyIlcC4GzzhjnhnAHe77G4Cf1L0KU0hluc8i0hx4HycJFPbzxpDFPqvqcVWtrKqhqhqKc12kt6oW5n5Os/O3/T1OawARqYxzqmh7PsaY17Kzz7uBqwFEpAFOIjiYr1HmrxlAf/fuodbAcVXdl5sVFrlTQ6qaKCIPAPNw7jj4WFU3isizQISqzgAm4DQft+FclOnnu4hzL5v7PAYoA3ztXhffraq9fRZ0LmVzn4uUbO7zPKCLiGwCkoChqlpoW7vZ3OdHgA9F5GGcC8cDCvOBnYh8iZPMK7vXPZ4GigOo6ns410GuAbYBp4A7c73NQvx5GWOMyQNF8dSQMcaYHLBEYIwxfs4SgTHG+DlLBMYY4+csERhjjJ+zRGAKJBFJEpFIj1doJvNG58H2JonIDndba9wnVHO6jo9EpKH7/okzpv2a2xjd9aR8LhtEZKaIVMhi/rDCXo3TeJ/dPmoKJBGJVtUyeT1vJuuYBPygqt+ISBfgVVVtmov15TqmrNYrIp8AW1X1+UzmH4BTdfWBvI7FFB3WIjCFgoiUcftRWCMiv4vIWZVGReQ8EVniccTc1h3fRUSWu8t+LSJZ/UAvAS52lx3irmuDiDzkjistIrNEZJ07vq87frGIhIvIS0CwG8dkd1q0+/8pItLDI+ZJInKDiASKyBgRWeXWmL8nGx/LctxiYyLS0t3HtSLyq4hc4j6J+yzQ142lrxv7xyLymztvehVbjb/xde1te9krvRfOU7GR7us7nKfgy7nTKuM8VZnSoo12//8I8KT7PhCn3lBlnB/20u74YcDIdLY3CbjBfX8jsBK4FPgdKI3zVPZGoDlwPfChx7Ll3f8vxu3zICUmj3lSYrwO+MR9XwKnimQwcDcwwh1fEogA6qQTZ7TH/n0NdHOHywHF3PedgGnu+wHAOI/lXwBuc99XwKlFVNrX37e9fPsqciUmTJFxWlXDUgZEpDjwgohcBSTjHAlXA/7xWGYV8LE77/eqGiki7XA6K1nmltYogXMknZ4xIjICp07NQJz6Nd+paowbw7dAW2Au8JqIvIxzOmlpDvZrDvCmiJQEugFLVPW0ezqqqYjc4M5XHqdY3I4zlg8WkUh3/zcD8z3m/0RE6uKUWSiewfa7AL1F5FF3OAio5a7L+ClLBKawuBWoAlyqqgniVBQN8pxBVZe4iaIHMElExgJHgfmqenM2tjFUVb9JGRCRq9ObSVW3itPXwTXAaBFZqKrPZmcnVDVWRBYDXYG+OB2tgNPb1GBVnZfFKk6rapiIlMKpv3M/8BZOBzyLVPU698L64gyWF+B6Vd2SnXiNf7BrBKawKA8ccJNAB+CsPpfF6Yd5v6p+CHyE093fCqCNiKSc8y8tIvWyuc2lwLUiUkpESuOc1lkqIucDp1T1c5xifun1GZvgtkzSMxWnUFhK6wKcH/X/piwjIvXcbaZLnd7m/gc8Iv+WUk8pRTzAY9aTOKfIUswDBovbPBKnKq3xc5YITGExGQgXkd+B/sAf6czTHlgnImtxjrbfVNWDOD+MX4rIepzTQvWzs0FVXYNz7eA3nGsGH6nqWqAJ8Jt7iuZpYHQ6i38ArE+5WHyGH3E6BlqgTveL4CSuTcAacTotf58sWuxuLOtxOmZ5BXjR3XfP5RYBDVMuFuO0HIq7sW10h42fs9tHjTHGz1mLwBhj/JwlAmOM8XOWCIwxxs9ZIjDGGD9nicAYY/ycJQJjjPFzlgiMMcbP/T+dBbqOrORwJwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS Statistic: 0.91\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Calculando a curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Calculando a estatística KS\n",
    "ks_statistic = np.max(tpr - fpr)\n",
    "ks_idx = np.argmax(tpr - fpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='True Positive Rate (TPR)')\n",
    "plt.plot(fpr, fpr, label='False Positive Rate (FPR)')\n",
    "plt.plot([fpr[ks_idx]], [tpr[ks_idx]], marker='o', markersize=5, color=\"red\")\n",
    "plt.text(fpr[ks_idx], tpr[ks_idx], f'KS={ks_statistic:.2f}', fontsize=12, verticalalignment='top')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Kolmogorov-Smirnov (KS) Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print(f\"KS Statistic: {ks_statistic:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos chegar nesses resultados, foram bons resultados mas vamos ver quais resultados conseguimos obter com outros algoritmos de classificações binárias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
